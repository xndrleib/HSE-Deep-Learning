{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zriTdjauH8iQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using pre-trained transformers (2pts)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rP1KFtvLlJHR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nYUNuyXMn5l9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kgb/opt/anaconda3/envs/conda_3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: decodestring() is a deprecated alias since Python 3.1, use decodebytes()\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "outputs = {house_name: classifier(data[house_name])[0]['label'] == 'POSITIVE' for house_name in data}\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodestring(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pa-8noIllRbZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9NxeG1Y5pwX1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.62004 the soviet union had its roots in the october revolution of 1917.\n",
      "P=0.33328 the soviet union had its roots in the october revolution of russia.\n",
      "P=0.00625 the soviet union had its roots in the october revolution of ukraine.\n",
      "P=0.00572 the soviet union had its roots in the october revolution of 1918.\n",
      "P=0.00445 the soviet union had its roots in the october revolution of bolsheviks.\n"
     ]
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "for hypo in mlm_model(f\"The Soviet Union had its roots in the October Revolution of {MASK}.\"):\n",
    "    print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HRux8Qp2hkXr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = pipeline('ner',\n",
    "                     model='dslim/bert-base-NER')\n",
    "\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hf57MRzSiSON",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'B-LOC', 'score': 0.7991049, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-LOC', 'score': 0.9511926, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'B-ORG', 'score': 0.998223, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'B-PER', 'score': 0.9997613, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.99978703, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.999646, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'B-PER', 'score': 0.9997831, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.99974823, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'B-LOC', 'score': 0.9997227, 'index': 85, 'word': 'Germany', 'start': 414, 'end': 421}, {'entity': 'B-PER', 'score': 0.9963128, 'index': 99, 'word': 'Phil', 'start': 471, 'end': 475}, {'entity': 'I-PER', 'score': 0.9889253, 'index': 100, 'word': '##ae', 'start': 475, 'end': 477}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KMJbV0QVsO0Q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZgSPHKPRxG6U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MJkbHxERyfL4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3502,  0.2246, -0.2345,  ..., -0.2232,  0.1730,  0.6747],\n",
      "         [-0.6097,  0.6892, -0.5512,  ..., -0.4814,  0.5322,  1.3833],\n",
      "         [ 0.1842,  0.4881,  0.2193,  ..., -0.2699,  0.2246,  0.7985],\n",
      "         ...,\n",
      "         [-0.4413,  0.2748, -0.0391,  ..., -0.0604, -0.4358,  0.1384],\n",
      "         [-0.5414,  0.4633,  0.0678,  ..., -0.1871, -0.5046,  0.2752],\n",
      "         [-0.3940,  0.6180,  0.2092,  ..., -0.2345, -0.4177,  0.3341]],\n",
      "\n",
      "        [[ 0.1622, -0.1154, -0.3894,  ..., -0.4180,  0.0138,  0.7644],\n",
      "         [ 0.6471,  0.3774, -0.4082,  ...,  0.0050,  0.5559,  0.4385],\n",
      "         [ 0.3351, -0.3158, -0.1178,  ...,  0.1348, -0.3143,  1.4409],\n",
      "         ...,\n",
      "         [ 1.2932, -0.1743, -0.5613,  ..., -0.2718, -0.1367,  0.4217],\n",
      "         [ 1.0305,  0.1708, -0.2985,  ...,  0.2097, -0.4627, -0.4277],\n",
      "         [ 1.0854,  0.1760, -0.0377,  ...,  0.3152, -0.5979, -0.3465]]]), pooler_output=tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fine-tuning for salary prediction (5 pts)\n",
    "\n",
    "Now let's put all this monstrosity to good use!\n",
    "\n",
    "Remember week5 when you've trained a convolutional neural network for salary prediction? Now let's see how transformers fare at this task.\n",
    "\n",
    "__The goal__ is to take one or more pre-trained models and fine-tune it for salary prediction. A good baseline solution would be to get RoBerta or T5 from [huggingface model list](https://huggingface.co/models) and fine-tune it to solve the task. After choosing the model, please take care to use the matching Tokenizer for preprocessing, as different models have different preprocessing requirements.\n",
    "\n",
    "\n",
    "There are no prompts this time: you will have to write everything from scratch. Although, feel free to reuse any code from the original salary prediction notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2021-11-12 16:46:45--  https://www.dropbox.com/s/r9d1f3ve471osob/Train_rev1.zip?dl=1\r\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18\r\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: /s/dl/r9d1f3ve471osob/Train_rev1.zip [following]\r\n",
      "--2021-11-12 16:46:45--  https://www.dropbox.com/s/dl/r9d1f3ve471osob/Train_rev1.zip\r\n",
      "Reusing existing connection to www.dropbox.com:443.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com/cd/0/get/BZ0Kj1_lLku1WLbMtMGuxrQmd4QZQim_oxRnAGTLORcI-WqbLgOOjesP2xG44-tNm_63Jy-uOmUfNAxVU-r0ps4PA6rs__Rp9INA7dH41U01iZmC4jN7Bv00k4PGKuHj3vcFnY-hU7AfcEMCcBTcVz99/file?dl=1# [following]\r\n",
      "--2021-11-12 16:46:46--  https://uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com/cd/0/get/BZ0Kj1_lLku1WLbMtMGuxrQmd4QZQim_oxRnAGTLORcI-WqbLgOOjesP2xG44-tNm_63Jy-uOmUfNAxVU-r0ps4PA6rs__Rp9INA7dH41U01iZmC4jN7Bv00k4PGKuHj3vcFnY-hU7AfcEMCcBTcVz99/file?dl=1\r\n",
      "Resolving uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com (uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com)... 162.125.70.15\r\n",
      "Connecting to uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com (uc4e67709f7986d95aecbb6a024d.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 128356352 (122M) [application/binary]\r\n",
      "Saving to: ‘data.zip’\r\n",
      "\r\n",
      "data.zip            100%[===================>] 122.41M  17.2MB/s    in 9.0s    \r\n",
      "\r\n",
      "2021-11-12 16:46:55 (13.6 MB/s) - ‘data.zip’ saved [128356352/128356352]\r\n",
      "\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Archive:  data.zip\r\n",
      "replace Train_rev1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\r\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://www.dropbox.com/s/r9d1f3ve471osob/Train_rev1.zip?dl=1' -O data.zip\n",
    "!unzip -e data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base').eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = roberta_model(**encoded_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "Engineering Systems Analyst\n",
      "HR Assistant\n",
      "Senior EC&I Engineer\n",
      "\n",
      "Matrix:\n",
      "tensor([[    0, 13929,  2961,  5778,  9821,     2,     1],\n",
      "        [    0, 16271,  6267,     2,     1,     1,     1],\n",
      "        [    0, 33867, 11270,   947,   100, 24379,     2]])\n",
      "\n",
      "Detokenized:\n",
      "<s>Engineering Systems Analyst</s><pad>\n",
      "<s>HR Assistant</s><pad><pad><pad>\n",
      "<s>Senior EC&I Engineer</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_info = tokenizer(data[\"Title\"][::100000].to_list(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(tokens_info['input_ids'], end='\\n\\n')\n",
    "print(\"Detokenized:\")\n",
    "print(*[tokenizer.decode(tokens_info['input_ids'][i]) for i in range(len(tokens_info['input_ids']))], sep='\\n', end='\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a pytorch-friendly dict from the batch data.\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "\n",
    "    batch = {}\n",
    "    for col in text_columns:\n",
    "        # batch[col] = tokenizer(data[col].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "        batch[col] = tokenizer(data[col].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "\n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Title': {'input_ids': tensor([[    0, 47666,    38,  5944,  4706, 25857,  4827,  1437,  6571,  1753,\n              2],\n         [    0, 45062,   783, 27592,   118,  7443, 33688,     2,     1,     1,\n              1],\n         [    0,  1121,  7248,  5293,  3105, 16688,     2,     1,     1,     1,\n              1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])},\n 'FullDescription': {'input_ids': tensor([[    0, 18387,    38,  ..., 17491,  4794,     2],\n         [    0, 27172,  8396,  ...,     1,     1,     1],\n         [    0,   170,    32,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]])},\n 'Categorical': array([[1., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n 'Log1pSalary': array([10.657283, 10.738156,  9.54524 ], dtype=float32)}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(data_train, 3, max_len=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding the head of the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:{}'.format(torch.cuda.current_device())) if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "    def forward(self, sample):\n",
    "        input_ids = sample['input_ids']\n",
    "        attention_mask = sample['attention_mask']\n",
    "        output = self.roberta_model(input_ids, attention_mask, return_dict=True)\n",
    "        return output[\"pooler_output\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "emb_model = Embeddings().eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, out_size=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(768, 384)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.dense2 = torch.nn.Linear(384, out_size)\n",
    "\n",
    "    def forward(self, sample):\n",
    "        emb = emb_model(sample)\n",
    "\n",
    "        h = self.dense1(emb)\n",
    "        h = nn.ReLU()(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.dense2(h)\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, out_size=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.embeddings = Embeddings().eval()\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(768, 384)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.dense2 = torch.nn.Linear(384, out_size)\n",
    "\n",
    "    def forward(self, sample):\n",
    "        emb = emb_model(sample)\n",
    "\n",
    "        h = self.dense1(emb)\n",
    "        h = nn.ReLU()(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.dense2(h)\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.title_encoder = TitleEncoder(out_size=64)\n",
    "        self.desc_encoder = DescriptionEncoder(out_size=64)\n",
    "\n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        self.dense1 = nn.Linear(n_cat_features, 300)\n",
    "        self.dense2 = nn.Linear(300, 64)\n",
    "\n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        self.dense_final = nn.Linear(192, 1)\n",
    "\n",
    "\n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "\n",
    "        # apply categorical encoder\n",
    "        cat_h = self.dense2(self.dense1(cat_features))\n",
    "\n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "\n",
    "        # ... and stack a few more layers at the top\n",
    "\n",
    "\n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "\n",
    "        return self.dense_final(joint_h)[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = FullNetwork().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# test it on one batch\n",
    "\n",
    "batch = generate_batch(data_train, 32)\n",
    "\n",
    "title_ix = batch[\"Title\"].to(device)\n",
    "desc_ix = batch[\"FullDescription\"].to(device)\n",
    "cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device)\n",
    "reference = torch.FloatTensor(batch[target_column]).to(device)\n",
    "\n",
    "prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "assert len(prediction.shape) == 1 and prediction.shape[0] == title_ix['input_ids'].shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def compute_loss(reference, prediction):\n",
    "    \"\"\"\n",
    "    Computes objective for minimization.\n",
    "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
    "    \"\"\"\n",
    "    return torch.mean((prediction - reference) ** 2)\n",
    "\n",
    "def compute_mae(reference, prediction):\n",
    "    \"\"\" Compute MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
    "    return torch.abs(torch.exp(reference - 1) - torch.exp(prediction - 1)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "loss = compute_loss(reference, prediction)\n",
    "dummy_grads = torch.autograd.grad(loss, model.parameters(), allow_unused=True, retain_graph=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "\n",
    "    irange = trange if verbose else range\n",
    "\n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, opt, batch_size, patience=5, num_epochs=10, model_name='model1', batches_per_epoch = 100, save=False):\n",
    "    train_loss_ar = []\n",
    "    val_loss_ar = []\n",
    "\n",
    "    trigger_times = 0\n",
    "    train_loss = train_mae = train_batches = 0\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "\n",
    "    prev_val = -np.Inf\n",
    "    val_min = +np.Inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)\n",
    "        for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "            title_ix = batch[\"Title\"].to(device)\n",
    "            desc_ix = batch[\"FullDescription\"].to(device)\n",
    "            cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device)\n",
    "            reference = torch.FloatTensor(batch[target_column]).to(device)\n",
    "\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "            loss = compute_loss(reference, prediction)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            train_batches += 1\n",
    "\n",
    "            train_loss_ar.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.train(False)\n",
    "            for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "                title_ix = batch[\"Title\"].to(device)\n",
    "                desc_ix = batch[\"FullDescription\"].to(device)\n",
    "                cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device)\n",
    "                reference = torch.FloatTensor(batch[target_column]).to(device)\n",
    "\n",
    "                prediction = model(title_ix, desc_ix, cat_features)\n",
    "                loss = compute_loss(reference, prediction)\n",
    "\n",
    "                val_loss += loss.data.cpu().numpy()\n",
    "                val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "                val_batches += 1\n",
    "\n",
    "                val_loss_ar.append(loss.item())\n",
    "\n",
    "            val_mean = np.mean(val_loss_ar[-batches_per_epoch * num_epochs // batch_size:])\n",
    "\n",
    "\n",
    "        if val_mean < val_min and save:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'val_mean': val_mean,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': opt.state_dict(),\n",
    "            }, 'models/' + model_name + f'_{epoch+1}.pth')\n",
    "\n",
    "            val_min = val_mean\n",
    "\n",
    "        if val_mean > prev_val:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping\\n')\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "        prev_val = val_mean\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss_ar[-batches_per_epoch * num_epochs // batch_size:])))\n",
    "        print(\"  validation loss: \\t\\t\\t{:.3f}\".format(\n",
    "            val_mean))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6884 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "batches_per_epoch = len(data_train) // batch_size\n",
    "\n",
    "\n",
    "train(model, opt, batch_size=batch_size, patience=3, num_epochs=num_epochs, model_name='model', batches_per_epoch = batches_per_epoch, save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}