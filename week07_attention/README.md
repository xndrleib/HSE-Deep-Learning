### Materials
* Lecture slides - [__here__](https://github.com/yandexdataschool/nlp_course/blob/d7df7ebe30c8bcf662ec246e36ac0ed8115236c2/resources/slides/nlp19_04_seq2seq_attention.pdf) (by Lena Voita)
* Our videos (russian): [lecture1](https://yadi.sk/i/CX1M4cKnTuC3kg), [lecture2](https://yadi.sk/i/81nP3AcDIrBE5g) [seminar](https://yadi.sk/i/b_64Rs1anbTx9A)
* Stanford lecture on attention and transformers (english) - [video](https://www.youtube.com/watch?v=5vcj8kSwBCY)
* Alternative CMU lectures - [seq2seq](https://www.youtube.com/watch?v=aHkgjfKvIhk&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE&index=20) and [attention](https://www.youtube.com/watch?v=ullLRKZ99qQ&index=21&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE)

### Practice
This time there's only a basic seminar
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/practical_dl/blob/fall21/week07_attention/seminar.ipynb)
