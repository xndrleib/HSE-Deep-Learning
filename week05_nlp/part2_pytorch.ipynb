{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Natural Language Processing with Deep Learning (7 points)\n",
    "\n",
    "Today we're gonna apply the newly learned DL tools for sequence processing to the task of predicting job salary.\n",
    "\n",
    "Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the assignment core (orignally written for theano/tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### About the challenge\n",
    "For starters, let's download the data from __[here](https://yadi.sk/d/vVEOWPFY3NruT7)__.\n",
    "\n",
    "You can also get it from the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (in that case, pick `Train_rev1.*`).\n",
    "\n",
    "\n",
    "Our task is to predict one number, __SalaryNormalized__, in the sense of minimizing __Mean Absolute Error__.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "To do so, our model ca access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
    "\n",
    "\n",
    "You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# !wget -O Train_rev1.zip https://yadi.sk/d/vVEOWPFY3NruT7\n",
    "# !unzip Train_rev1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>59920957</td>\n",
       "      <td>Customer Relations/ Membership Sales Manager –...</td>\n",
       "      <td>We are looking for a person to support the Gen...</td>\n",
       "      <td>South Norwood</td>\n",
       "      <td>Norwood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Fusion</td>\n",
       "      <td>Travel Jobs</td>\n",
       "      <td>Up to 18,500 per annum</td>\n",
       "      <td>18500</td>\n",
       "      <td>leisurejobs.com</td>\n",
       "      <td>9.825580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156622</th>\n",
       "      <td>71096973</td>\n",
       "      <td>Registered General Nurse (RGN; RN)  York</td>\n",
       "      <td>Registered General Nurse (RGN; RN) needed for ...</td>\n",
       "      <td>York</td>\n",
       "      <td>York</td>\n",
       "      <td>part_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The A24 Group</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>20.09 - 27.00/Hour</td>\n",
       "      <td>45206</td>\n",
       "      <td>staffnurse.com</td>\n",
       "      <td>10.719007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29839</th>\n",
       "      <td>68058322</td>\n",
       "      <td>CHOKE VALVE SALES ENGINEER</td>\n",
       "      <td>CHOKE VALVE SALES ENGINEER YOU MUST HAVE PREVI...</td>\n",
       "      <td>West Yorkshire Yorkshire</td>\n",
       "      <td>West Yorkshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Code Blue Recruitment</td>\n",
       "      <td>Sales Jobs</td>\n",
       "      <td>From 28,000 to 31,000 per annum</td>\n",
       "      <td>29500</td>\n",
       "      <td>totaljobs.com</td>\n",
       "      <td>10.292179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "3849    59920957  Customer Relations/ Membership Sales Manager –...   \n",
       "156622  71096973           Registered General Nurse (RGN; RN)  York   \n",
       "29839   68058322                         CHOKE VALVE SALES ENGINEER   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "3849    We are looking for a person to support the Gen...   \n",
       "156622  Registered General Nurse (RGN; RN) needed for ...   \n",
       "29839   CHOKE VALVE SALES ENGINEER YOU MUST HAVE PREVI...   \n",
       "\n",
       "                     LocationRaw LocationNormalized ContractType ContractTime  \\\n",
       "3849               South Norwood            Norwood          NaN    permanent   \n",
       "156622                      York               York    part_time          NaN   \n",
       "29839   West Yorkshire Yorkshire     West Yorkshire          NaN    permanent   \n",
       "\n",
       "                      Company                   Category  \\\n",
       "3849                   Fusion                Travel Jobs   \n",
       "156622          The A24 Group  Healthcare & Nursing Jobs   \n",
       "29839   Code Blue Recruitment                 Sales Jobs   \n",
       "\n",
       "                              SalaryRaw  SalaryNormalized       SourceName  \\\n",
       "3849             Up to 18,500 per annum             18500  leisurejobs.com   \n",
       "156622               20.09 - 27.00/Hour             45206   staffnurse.com   \n",
       "29839   From 28,000 to 31,000 per annum             29500    totaljobs.com   \n",
       "\n",
       "        Log1pSalary  \n",
       "3849       9.825580  \n",
       "156622    10.719007  \n",
       "29839     10.292179  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The NLP part\n",
    "\n",
    "To even begin training our neural network, we're gonna need to preprocess the text features: tokenize it and build the token vocabularies.\n",
    "\n",
    "Since it is not an NLP course, we're gonna use simple built-in NLTK tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "0         Engineering Systems Analyst\n",
      "100000                   HR Assistant\n",
      "200000           Senior EC&I Engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(lambda l: ' '.join(tokenizer.tokenize(str(l).lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "0         engineering systems analyst\n",
      "100000                   hr assistant\n",
      "200000         senior ec & i engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's see how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "for line in range(len(data)):\n",
    "    token_counts.update(data['Title'][line].split())\n",
    "    token_counts.update(data['FullDescription'][line].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 202704\n",
      "('and', 2657388)\n",
      "('.', 2523216)\n",
      "(',', 2318606)\n",
      "('the', 2080994)\n",
      "('to', 2019884)\n",
      "...\n",
      "('stephanietraveltraderecruitmnt', 1)\n",
      "('ruabon', 1)\n",
      "('lowehays', 1)\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Counts')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many words are there for each count\n",
    "\n",
    "_=plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Task 1.1__ Get a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = [token for token in token_counts if token_counts[token] >= min_count]\n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens left: 34158\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens left:\", len(tokens))\n",
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Task 1.2__ Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token_to_id = {token: tokens.index(token) for token in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into torch-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[    2     3     4     1     1]\n",
      " [  550  2380     1     1     1]\n",
      " [  320 10714   392   307    32]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement tf-idf, target averaging or pseudo-counter-based encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The data science part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  220291\n",
      "Validation size =  24477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a pytorch-friendly dict from the batch data.\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "    \n",
    "    batch = {}\n",
    "    for col in text_columns:\n",
    "        batch[col] = as_matrix(data[col].values, max_len)\n",
    "    \n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': array([[ 2501,   266,     1,     1,     1,     1],\n",
       "        [  421,    23,   901,  1783,   654,   655],\n",
       "        [ 1526,  2380, 16440,  4723,     1,     1]], dtype=int32),\n",
       " 'FullDescription': array([[  230,    66,    19,   420,  2501,  3722,    43,   448,    47,\n",
       "            56],\n",
       "        [   49,   421,    23,   901,    14,   433,   103,   129,   224,\n",
       "           226],\n",
       "        [ 1526,  2380, 16440,  4723,     8,   410,     7,  2225,    65,\n",
       "           103]], dtype=int32),\n",
       " 'Categorical': array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'Log1pSalary': array([10.477316, 10.71444 ,  9.615872], dtype=float32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(data_train, 3, max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Finally, let's talk deep learning\n",
    "\n",
    "Out model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/Practical_DL/raw/master/homework04/conv_salary_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By default, both text vectorizers shall use 1d convolutions, followed by global pooling over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:{}'.format(torch.cuda.current_device())) if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dense(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = TitleEncoder(out_size=64)\n",
    "\n",
    "dummy_x = torch.LongTensor(generate_batch(data_train, 3)['Title'])\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Task 2.1__ Create description encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "\n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dense(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine too\n"
     ]
    }
   ],
   "source": [
    "desc_encoder = DescriptionEncoder(out_size=64)\n",
    "\n",
    "dummy_x = torch.LongTensor(generate_batch(data_train, 3)['FullDescription'])\n",
    "dummy_v = desc_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "del desc_encoder\n",
    "print(\"Seems fine too\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Task 2.2__ Build one network ~~to rule them all~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoder(n_tokens, out_size=64)\n",
    "        self.desc_encoder = DescriptionEncoder(n_tokens, out_size=64)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        self.dense1 = nn.Linear(n_cat_features, 100)\n",
    "        self.dense2 = nn.Linear(100, 64)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        self.dense_final = nn.Linear(192, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.dense2(self.dense1(cat_features))\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return self.dense_final(joint_h)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = FullNetwork().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test it on one batch\n",
    "\n",
    "batch = generate_batch(data_train, 32)\n",
    "\n",
    "title_ix = torch.LongTensor(batch[\"Title\"]).to(device)\n",
    "desc_ix = torch.LongTensor(batch[\"FullDescription\"]).to(device)\n",
    "cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device)\n",
    "reference = torch.FloatTensor(batch[target_column]).to(device)\n",
    "\n",
    "prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "assert len(prediction.shape) == 1 and prediction.shape[0] == title_ix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(reference, prediction):\n",
    "    \"\"\"\n",
    "    Computes objective for minimization.\n",
    "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
    "    \"\"\"\n",
    "    return torch.mean((prediction - reference) ** 2)\n",
    "\n",
    "def compute_mae(reference, prediction):\n",
    "    \"\"\" Compute MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
    "    return torch.abs(torch.exp(reference - 1) - torch.exp(prediction - 1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = compute_loss(reference, prediction)\n",
    "dummy_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "for grad in dummy_grads:\n",
    "    assert grad is not None and not (grad == 0).all(), \"Some model parameters received zero grads. \" \\\n",
    "                                                       \"Double-check that your model uses all it's layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = trange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, opt, batch_size, patience=5, num_epochs=10, model_name='model1', batches_per_epoch = 100, save=False):\n",
    "    train_loss_ar = []\n",
    "    val_loss_ar = []\n",
    "\n",
    "    trigger_times = 0\n",
    "    train_loss = train_mae = train_batches = 0  \n",
    "    val_loss = val_mae = val_batches = 0\n",
    "\n",
    "    prev_val = -np.Inf\n",
    "    val_min = +np.Inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)\n",
    "        for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "            title_ix = torch.LongTensor(batch[\"Title\"]).to(device, non_blocking=True)\n",
    "            desc_ix = torch.LongTensor(batch[\"FullDescription\"]).to(device, non_blocking=True)\n",
    "            cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device, non_blocking=True)\n",
    "            reference = torch.FloatTensor(batch[target_column]).to(device, non_blocking=True)\n",
    "\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "            loss = compute_loss(reference, prediction)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            train_batches += 1\n",
    "            \n",
    "            train_loss_ar.append(loss.item())\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.train(False)\n",
    "            for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "                title_ix = torch.LongTensor(batch[\"Title\"]).to(device, non_blocking=True)\n",
    "                desc_ix = torch.LongTensor(batch[\"FullDescription\"]).to(device, non_blocking=True)\n",
    "                cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device, non_blocking=True)\n",
    "                reference = torch.FloatTensor(batch[target_column]).to(device, non_blocking=True)\n",
    "\n",
    "                prediction = model(title_ix, desc_ix, cat_features)\n",
    "                loss = compute_loss(reference, prediction)\n",
    "\n",
    "                val_loss += loss.data.cpu().numpy()\n",
    "                val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "                val_batches += 1\n",
    "                \n",
    "                val_loss_ar.append(loss.item())\n",
    "                \n",
    "            val_mean = np.mean(val_loss_ar[-batches_per_epoch * num_epochs // batch_size:])\n",
    "\n",
    "\n",
    "        if val_mean < val_min and save:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'val_mean': val_mean,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': opt.state_dict(),\n",
    "            }, 'models/' + model_name + f'_{epoch+1}.pth')\n",
    "\n",
    "            val_min = val_mean\n",
    "\n",
    "        if val_mean > prev_val:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping\\n')\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "        prev_val = val_mean\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss_ar[-batches_per_epoch * num_epochs // batch_size:])))\n",
    "        print(\"  validation loss: \\t\\t\\t{:.3f}\".format(\n",
    "            val_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.45it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 108.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 9.008s\n",
      "  training loss (in-iteration): \t8.725797\n",
      "  validation loss: \t\t\t62.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.60it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 100 took 9.303s\n",
      "  training loss (in-iteration): \t4.607686\n",
      "  validation loss: \t\t\t41.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.23it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 100 took 9.339s\n",
      "  training loss (in-iteration): \t3.194679\n",
      "  validation loss: \t\t\t32.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.90it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 100 took 9.247s\n",
      "  training loss (in-iteration): \t0.401203\n",
      "  validation loss: \t\t\t28.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.50it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 100 took 9.309s\n",
      "  training loss (in-iteration): \t0.318960\n",
      "  validation loss: \t\t\t25.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.42it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 100 took 9.210s\n",
      "  training loss (in-iteration): \t0.272481\n",
      "  validation loss: \t\t\t23.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.44it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 100 took 9.302s\n",
      "  training loss (in-iteration): \t0.237620\n",
      "  validation loss: \t\t\t21.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.06it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 100 took 9.367s\n",
      "  training loss (in-iteration): \t0.221719\n",
      "  validation loss: \t\t\t21.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.77it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 100 took 9.286s\n",
      "  training loss (in-iteration): \t0.211306\n",
      "  validation loss: \t\t\t20.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.85it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 100 took 9.345s\n",
      "  training loss (in-iteration): \t0.205305\n",
      "  validation loss: \t\t\t19.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.04it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 100 took 9.319s\n",
      "  training loss (in-iteration): \t0.192581\n",
      "  validation loss: \t\t\t18.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.62it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 100 took 9.409s\n",
      "  training loss (in-iteration): \t0.185202\n",
      "  validation loss: \t\t\t17.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.88it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 100 took 9.340s\n",
      "  training loss (in-iteration): \t0.178517\n",
      "  validation loss: \t\t\t17.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 100 took 9.236s\n",
      "  training loss (in-iteration): \t0.174709\n",
      "  validation loss: \t\t\t17.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.28it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 100 took 9.376s\n",
      "  training loss (in-iteration): \t0.168282\n",
      "  validation loss: \t\t\t18.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.06it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 100 took 9.478s\n",
      "  training loss (in-iteration): \t0.163430\n",
      "  validation loss: \t\t\t14.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.67it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 100 took 9.299s\n",
      "  training loss (in-iteration): \t0.153806\n",
      "  validation loss: \t\t\t16.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.26it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 100 took 9.435s\n",
      "  training loss (in-iteration): \t0.148035\n",
      "  validation loss: \t\t\t14.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.56it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 100 took 9.177s\n",
      "  training loss (in-iteration): \t0.144598\n",
      "  validation loss: \t\t\t15.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.27it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 100 took 9.257s\n",
      "  training loss (in-iteration): \t0.143483\n",
      "  validation loss: \t\t\t15.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.95it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 100 took 9.327s\n",
      "  training loss (in-iteration): \t0.141206\n",
      "  validation loss: \t\t\t14.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.86it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 100 took 9.256s\n",
      "  training loss (in-iteration): \t0.135939\n",
      "  validation loss: \t\t\t15.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.35it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 100.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 100 took 9.526s\n",
      "  training loss (in-iteration): \t0.132449\n",
      "  validation loss: \t\t\t12.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.68it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 100 took 9.290s\n",
      "  training loss (in-iteration): \t0.127041\n",
      "  validation loss: \t\t\t15.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.34it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 100 took 9.362s\n",
      "  training loss (in-iteration): \t0.126487\n",
      "  validation loss: \t\t\t13.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.43it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 100 took 9.294s\n",
      "  training loss (in-iteration): \t0.129504\n",
      "  validation loss: \t\t\t20.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.43it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 100 took 9.310s\n",
      "  training loss (in-iteration): \t0.130117\n",
      "  validation loss: \t\t\t13.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.71it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 100 took 9.297s\n",
      "  training loss (in-iteration): \t0.126214\n",
      "  validation loss: \t\t\t11.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.12it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 100 took 9.332s\n",
      "  training loss (in-iteration): \t0.119195\n",
      "  validation loss: \t\t\t11.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.48it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 100 took 9.378s\n",
      "  training loss (in-iteration): \t0.116090\n",
      "  validation loss: \t\t\t12.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.07it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 100 took 9.228s\n",
      "  training loss (in-iteration): \t0.118399\n",
      "  validation loss: \t\t\t11.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.17it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 100 took 9.380s\n",
      "  training loss (in-iteration): \t0.118906\n",
      "  validation loss: \t\t\t11.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.62it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 100 took 9.297s\n",
      "  training loss (in-iteration): \t0.116077\n",
      "  validation loss: \t\t\t11.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.46it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 100 took 9.324s\n",
      "  training loss (in-iteration): \t0.111489\n",
      "  validation loss: \t\t\t12.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.69it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 100 took 9.257s\n",
      "  training loss (in-iteration): \t0.112208\n",
      "  validation loss: \t\t\t13.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.54it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 100 took 9.371s\n",
      "  training loss (in-iteration): \t0.111297\n",
      "  validation loss: \t\t\t11.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.08it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 100 took 9.283s\n",
      "  training loss (in-iteration): \t0.114152\n",
      "  validation loss: \t\t\t11.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.69it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 100 took 9.234s\n",
      "  training loss (in-iteration): \t0.113193\n",
      "  validation loss: \t\t\t11.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.75it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 of 100 took 9.294s\n",
      "  training loss (in-iteration): \t0.112690\n",
      "  validation loss: \t\t\t11.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.86it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 of 100 took 9.238s\n",
      "  training loss (in-iteration): \t0.113933\n",
      "  validation loss: \t\t\t14.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.69it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 100 took 9.405s\n",
      "  training loss (in-iteration): \t0.113165\n",
      "  validation loss: \t\t\t11.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.93it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 of 100 took 9.205s\n",
      "  training loss (in-iteration): \t0.110240\n",
      "  validation loss: \t\t\t13.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.73it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 100.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 of 100 took 9.562s\n",
      "  training loss (in-iteration): \t0.109145\n",
      "  validation loss: \t\t\t10.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.56it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 100 took 9.402s\n",
      "  training loss (in-iteration): \t0.107381\n",
      "  validation loss: \t\t\t10.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.46it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 of 100 took 9.309s\n",
      "  training loss (in-iteration): \t0.110192\n",
      "  validation loss: \t\t\t10.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.07it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 of 100 took 9.369s\n",
      "  training loss (in-iteration): \t0.104045\n",
      "  validation loss: \t\t\t9.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.28it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 100 took 9.255s\n",
      "  training loss (in-iteration): \t0.100203\n",
      "  validation loss: \t\t\t12.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.27it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 100 took 9.236s\n",
      "  training loss (in-iteration): \t0.100185\n",
      "  validation loss: \t\t\t15.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.86it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 of 100 took 9.223s\n",
      "  training loss (in-iteration): \t0.106575\n",
      "  validation loss: \t\t\t12.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.86it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 of 100 took 9.327s\n",
      "  training loss (in-iteration): \t0.107347\n",
      "  validation loss: \t\t\t10.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.48it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 of 100 took 9.427s\n",
      "  training loss (in-iteration): \t0.105314\n",
      "  validation loss: \t\t\t9.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.07it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 of 100 took 9.286s\n",
      "  training loss (in-iteration): \t0.101961\n",
      "  validation loss: \t\t\t12.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.94it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 of 100 took 9.199s\n",
      "  training loss (in-iteration): \t0.102604\n",
      "  validation loss: \t\t\t13.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.69it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 of 100 took 9.285s\n",
      "  training loss (in-iteration): \t0.104824\n",
      "  validation loss: \t\t\t10.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.72it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 of 100 took 9.378s\n",
      "  training loss (in-iteration): \t0.105578\n",
      "  validation loss: \t\t\t9.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.24it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 of 100 took 9.516s\n",
      "  training loss (in-iteration): \t0.101687\n",
      "  validation loss: \t\t\t9.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.32it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 of 100 took 9.338s\n",
      "  training loss (in-iteration): \t0.097339\n",
      "  validation loss: \t\t\t10.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.72it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 of 100 took 9.204s\n",
      "  training loss (in-iteration): \t0.099137\n",
      "  validation loss: \t\t\t13.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.18it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 of 100 took 9.248s\n",
      "  training loss (in-iteration): \t0.099548\n",
      "  validation loss: \t\t\t11.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.24it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 of 100 took 9.391s\n",
      "  training loss (in-iteration): \t0.098102\n",
      "  validation loss: \t\t\t9.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.06it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 of 100 took 9.185s\n",
      "  training loss (in-iteration): \t0.095146\n",
      "  validation loss: \t\t\t11.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.15it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 of 100 took 9.298s\n",
      "  training loss (in-iteration): \t0.094688\n",
      "  validation loss: \t\t\t9.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.86it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 of 100 took 9.227s\n",
      "  training loss (in-iteration): \t0.096264\n",
      "  validation loss: \t\t\t9.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.23it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 of 100 took 9.348s\n",
      "  training loss (in-iteration): \t0.095784\n",
      "  validation loss: \t\t\t9.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.16it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 of 100 took 9.234s\n",
      "  training loss (in-iteration): \t0.095210\n",
      "  validation loss: \t\t\t9.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.53it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 of 100 took 9.151s\n",
      "  training loss (in-iteration): \t0.092225\n",
      "  validation loss: \t\t\t11.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.05it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 of 100 took 9.190s\n",
      "  training loss (in-iteration): \t0.093926\n",
      "  validation loss: \t\t\t10.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.79it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 of 100 took 9.168s\n",
      "  training loss (in-iteration): \t0.094115\n",
      "  validation loss: \t\t\t10.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.24it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 of 100 took 9.167s\n",
      "  training loss (in-iteration): \t0.097833\n",
      "  validation loss: \t\t\t13.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.73it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 of 100 took 9.245s\n",
      "  training loss (in-iteration): \t0.097909\n",
      "  validation loss: \t\t\t9.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.83it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 of 100 took 9.352s\n",
      "  training loss (in-iteration): \t0.095469\n",
      "  validation loss: \t\t\t11.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.32it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 of 100 took 9.449s\n",
      "  training loss (in-iteration): \t0.092462\n",
      "  validation loss: \t\t\t8.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.19it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 101.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 of 100 took 9.463s\n",
      "  training loss (in-iteration): \t0.090343\n",
      "  validation loss: \t\t\t9.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.37it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 of 100 took 9.413s\n",
      "  training loss (in-iteration): \t0.088705\n",
      "  validation loss: \t\t\t8.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.32it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 of 100 took 9.231s\n",
      "  training loss (in-iteration): \t0.087846\n",
      "  validation loss: \t\t\t8.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.90it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 of 100 took 9.210s\n",
      "  training loss (in-iteration): \t0.088745\n",
      "  validation loss: \t\t\t8.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.62it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 of 100 took 9.292s\n",
      "  training loss (in-iteration): \t0.089638\n",
      "  validation loss: \t\t\t8.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.77it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 of 100 took 9.234s\n",
      "  training loss (in-iteration): \t0.094727\n",
      "  validation loss: \t\t\t10.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.22it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 of 100 took 9.185s\n",
      "  training loss (in-iteration): \t0.095235\n",
      "  validation loss: \t\t\t12.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.04it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 of 100 took 9.229s\n",
      "  training loss (in-iteration): \t0.093171\n",
      "  validation loss: \t\t\t16.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.54it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 of 100 took 9.395s\n",
      "  training loss (in-iteration): \t0.091811\n",
      "  validation loss: \t\t\t8.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.70it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 100 took 9.314s\n",
      "  training loss (in-iteration): \t0.088677\n",
      "  validation loss: \t\t\t9.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.08it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 of 100 took 9.326s\n",
      "  training loss (in-iteration): \t0.089250\n",
      "  validation loss: \t\t\t8.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.23it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 of 100 took 9.208s\n",
      "  training loss (in-iteration): \t0.085846\n",
      "  validation loss: \t\t\t9.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.18it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 of 100 took 9.174s\n",
      "  training loss (in-iteration): \t0.086940\n",
      "  validation loss: \t\t\t9.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.46it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 of 100 took 9.180s\n",
      "  training loss (in-iteration): \t0.085129\n",
      "  validation loss: \t\t\t9.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.64it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 of 100 took 9.298s\n",
      "  training loss (in-iteration): \t0.086731\n",
      "  validation loss: \t\t\t8.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.12it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 of 100 took 9.213s\n",
      "  training loss (in-iteration): \t0.084325\n",
      "  validation loss: \t\t\t11.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.68it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 of 100 took 9.329s\n",
      "  training loss (in-iteration): \t0.083021\n",
      "  validation loss: \t\t\t8.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.34it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 of 100 took 9.257s\n",
      "  training loss (in-iteration): \t0.084044\n",
      "  validation loss: \t\t\t9.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.66it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 of 100 took 9.307s\n",
      "  training loss (in-iteration): \t0.084742\n",
      "  validation loss: \t\t\t8.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.50it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 of 100 took 9.224s\n",
      "  training loss (in-iteration): \t0.083707\n",
      "  validation loss: \t\t\t9.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.83it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 of 100 took 9.183s\n",
      "  training loss (in-iteration): \t0.084969\n",
      "  validation loss: \t\t\t8.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.36it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 of 100 took 9.171s\n",
      "  training loss (in-iteration): \t0.086838\n",
      "  validation loss: \t\t\t8.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.08it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 of 100 took 9.122s\n",
      "  training loss (in-iteration): \t0.087778\n",
      "  validation loss: \t\t\t9.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.54it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 of 100 took 9.273s\n",
      "  training loss (in-iteration): \t0.084316\n",
      "  validation loss: \t\t\t8.433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.67it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 of 100 took 9.198s\n",
      "  training loss (in-iteration): \t0.081507\n",
      "  validation loss: \t\t\t8.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.25it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 104.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 of 100 took 9.177s\n",
      "  training loss (in-iteration): \t0.081401\n",
      "  validation loss: \t\t\t10.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.88it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 103.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 of 100 took 9.193s\n",
      "  training loss (in-iteration): \t0.084645\n",
      "  validation loss: \t\t\t8.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.15it/s]\n",
      "100%|██████████| 765/765 [00:07<00:00, 102.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 of 100 took 9.292s\n",
      "  training loss (in-iteration): \t0.088730\n",
      "  validation loss: \t\t\t11.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "train(model, opt, batch_size=batch_size, patience=5, num_epochs=num_epochs, model_name='model1', batches_per_epoch = 100, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:07<00:00, 106.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.11536\n",
      "\tMAE:\t3557.39676\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def final_eval(model):\n",
    "    print(\"Final eval:\")\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "            title_ix = torch.LongTensor(batch[\"Title\"]).to(device, non_blocking=True)\n",
    "            desc_ix = torch.LongTensor(batch[\"FullDescription\"]).to(device, non_blocking=True)\n",
    "            cat_features = torch.FloatTensor(batch[\"Categorical\"]).to(device, non_blocking=True)\n",
    "            reference = torch.FloatTensor(batch[target_column]).to(device, non_blocking=True)\n",
    "\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "\n",
    "            val_loss += loss.data.cpu().numpy()\n",
    "            val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            val_batches += 1\n",
    "\n",
    "    print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "final_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download Google's trained Word2Vec model\n",
    "\n",
    "# import requests\n",
    "\n",
    "# def download_file_from_google_drive(id, destination):\n",
    "#     URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "#     session = requests.Session()\n",
    "\n",
    "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "#     token = get_confirm_token(response)\n",
    "\n",
    "#     if token:\n",
    "#         params = { 'id' : id, 'confirm' : token }\n",
    "#         response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "#     save_response_content(response, destination)    \n",
    "\n",
    "# def get_confirm_token(response):\n",
    "#     for key, value in response.cookies.items():\n",
    "#         if key.startswith('download_warning'):\n",
    "#             return value\n",
    "\n",
    "#     return None\n",
    "\n",
    "# def save_response_content(response, destination):\n",
    "#     CHUNK_SIZE = 32768\n",
    "\n",
    "#     with open(destination, \"wb\") as f:\n",
    "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
    "#             if chunk: # filter out keep-alive new chunks\n",
    "#                 f.write(chunk)\n",
    "                \n",
    "# file_id = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
    "# destination = 'google_vectors.bin.gz'\n",
    "# download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "embeddings = KeyedVectors.load_word2vec_format('google_vectors.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict, dimension):\n",
    "  embedding_matrix=np.zeros((len(word_index)+1,dimension))\n",
    " \n",
    "  for word,index in word_index.items():\n",
    "    if word in embedding_dict:\n",
    "      embedding_matrix[index]=embedding_dict[word]\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(token_to_id, embedding_dict=embeddings, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TitleEncoderEmb(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "        self.emb.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32)\n",
    "        )\n",
    "        self.emb.weight.requires_grad=False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dense(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DescriptionEncoderEmb(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "        self.emb.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32)\n",
    "        )\n",
    "        self.emb.weight.requires_grad=False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "\n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dense(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetworkEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoderEmb(n_tokens, out_size=64)\n",
    "        self.desc_encoder = DescriptionEncoderEmb(n_tokens, out_size=64)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        self.dense1 = nn.Linear(n_cat_features, 300)\n",
    "        self.dense2 = nn.Linear(300, 64)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        self.dense_final = nn.Linear(192, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.dense2(self.dense1(cat_features))\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return self.dense_final(joint_h)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.32it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 70.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 13.112s\n",
      "  training loss (in-iteration): \t11.545213\n",
      "  validation loss: \t\t\t67.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.84it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 100 took 12.050s\n",
      "  training loss (in-iteration): \t6.031620\n",
      "  validation loss: \t\t\t35.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.53it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 80.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 100 took 11.476s\n",
      "  training loss (in-iteration): \t4.121728\n",
      "  validation loss: \t\t\t25.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.87it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 81.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 100 took 11.425s\n",
      "  training loss (in-iteration): \t0.366818\n",
      "  validation loss: \t\t\t21.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.55it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 73.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 100 took 12.427s\n",
      "  training loss (in-iteration): \t0.248622\n",
      "  validation loss: \t\t\t19.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.27it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 100 took 12.267s\n",
      "  training loss (in-iteration): \t0.202378\n",
      "  validation loss: \t\t\t15.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.17it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 100 took 11.923s\n",
      "  training loss (in-iteration): \t0.174562\n",
      "  validation loss: \t\t\t14.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.71it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 100 took 11.812s\n",
      "  training loss (in-iteration): \t0.153180\n",
      "  validation loss: \t\t\t13.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.56it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 73.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 100 took 12.536s\n",
      "  training loss (in-iteration): \t0.139768\n",
      "  validation loss: \t\t\t12.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.95it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 100 took 11.960s\n",
      "  training loss (in-iteration): \t0.130203\n",
      "  validation loss: \t\t\t12.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.57it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 100 took 11.770s\n",
      "  training loss (in-iteration): \t0.127445\n",
      "  validation loss: \t\t\t12.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.28it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 80.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 100 took 11.474s\n",
      "  training loss (in-iteration): \t0.127970\n",
      "  validation loss: \t\t\t11.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 57.13it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 74.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 100 took 12.175s\n",
      "  training loss (in-iteration): \t0.123803\n",
      "  validation loss: \t\t\t11.433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.81it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 100 took 11.946s\n",
      "  training loss (in-iteration): \t0.122170\n",
      "  validation loss: \t\t\t11.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.07it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 100 took 11.654s\n",
      "  training loss (in-iteration): \t0.116307\n",
      "  validation loss: \t\t\t10.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.89it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 100 took 11.709s\n",
      "  training loss (in-iteration): \t0.116984\n",
      "  validation loss: \t\t\t10.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.81it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 100 took 11.802s\n",
      "  training loss (in-iteration): \t0.117869\n",
      "  validation loss: \t\t\t11.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.43it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 100 took 12.135s\n",
      "  training loss (in-iteration): \t0.114366\n",
      "  validation loss: \t\t\t10.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.29it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 100 took 11.822s\n",
      "  training loss (in-iteration): \t0.115370\n",
      "  validation loss: \t\t\t10.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.92it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 100 took 11.753s\n",
      "  training loss (in-iteration): \t0.111760\n",
      "  validation loss: \t\t\t13.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.45it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 74.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 100 took 12.318s\n",
      "  training loss (in-iteration): \t0.110409\n",
      "  validation loss: \t\t\t9.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.72it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 100 took 11.990s\n",
      "  training loss (in-iteration): \t0.109077\n",
      "  validation loss: \t\t\t9.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.18it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 100 took 11.597s\n",
      "  training loss (in-iteration): \t0.110293\n",
      "  validation loss: \t\t\t19.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.21it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 100 took 11.657s\n",
      "  training loss (in-iteration): \t0.109568\n",
      "  validation loss: \t\t\t13.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.34it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 100 took 11.992s\n",
      "  training loss (in-iteration): \t0.104091\n",
      "  validation loss: \t\t\t16.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.88it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 100 took 12.084s\n",
      "  training loss (in-iteration): \t0.104591\n",
      "  validation loss: \t\t\t11.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.14it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 100 took 11.952s\n",
      "  training loss (in-iteration): \t0.105026\n",
      "  validation loss: \t\t\t9.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.18it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 100 took 11.819s\n",
      "  training loss (in-iteration): \t0.104626\n",
      "  validation loss: \t\t\t9.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.61it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 73.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 100 took 12.395s\n",
      "  training loss (in-iteration): \t0.099421\n",
      "  validation loss: \t\t\t9.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.76it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 100 took 12.253s\n",
      "  training loss (in-iteration): \t0.095300\n",
      "  validation loss: \t\t\t10.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.01it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 100 took 11.727s\n",
      "  training loss (in-iteration): \t0.094965\n",
      "  validation loss: \t\t\t11.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.54it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 100 took 11.774s\n",
      "  training loss (in-iteration): \t0.096623\n",
      "  validation loss: \t\t\t8.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.46it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 100 took 11.993s\n",
      "  training loss (in-iteration): \t0.096295\n",
      "  validation loss: \t\t\t9.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.65it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 100 took 12.087s\n",
      "  training loss (in-iteration): \t0.094610\n",
      "  validation loss: \t\t\t9.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.97it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 100 took 12.007s\n",
      "  training loss (in-iteration): \t0.090500\n",
      "  validation loss: \t\t\t8.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.88it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 100 took 11.972s\n",
      "  training loss (in-iteration): \t0.091902\n",
      "  validation loss: \t\t\t8.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.11it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 100 took 12.180s\n",
      "  training loss (in-iteration): \t0.092253\n",
      "  validation loss: \t\t\t8.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.18it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 100 took 12.024s\n",
      "  training loss (in-iteration): \t0.096163\n",
      "  validation loss: \t\t\t8.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.08it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 of 100 took 11.886s\n",
      "  training loss (in-iteration): \t0.094118\n",
      "  validation loss: \t\t\t10.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.47it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 of 100 took 11.463s\n",
      "  training loss (in-iteration): \t0.095299\n",
      "  validation loss: \t\t\t10.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.97it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 100 took 11.612s\n",
      "  training loss (in-iteration): \t0.091313\n",
      "  validation loss: \t\t\t9.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.03it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 of 100 took 12.171s\n",
      "  training loss (in-iteration): \t0.093220\n",
      "  validation loss: \t\t\t8.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.82it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 of 100 took 11.577s\n",
      "  training loss (in-iteration): \t0.094106\n",
      "  validation loss: \t\t\t12.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.33it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 100 took 11.618s\n",
      "  training loss (in-iteration): \t0.095620\n",
      "  validation loss: \t\t\t8.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.92it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 of 100 took 11.503s\n",
      "  training loss (in-iteration): \t0.094217\n",
      "  validation loss: \t\t\t12.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.91it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 of 100 took 12.082s\n",
      "  training loss (in-iteration): \t0.096907\n",
      "  validation loss: \t\t\t8.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.76it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 100 took 11.884s\n",
      "  training loss (in-iteration): \t0.097249\n",
      "  validation loss: \t\t\t8.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.01it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 100 took 11.565s\n",
      "  training loss (in-iteration): \t0.094275\n",
      "  validation loss: \t\t\t8.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.27it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 of 100 took 11.723s\n",
      "  training loss (in-iteration): \t0.089085\n",
      "  validation loss: \t\t\t9.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.28it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 of 100 took 11.946s\n",
      "  training loss (in-iteration): \t0.086123\n",
      "  validation loss: \t\t\t8.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.06it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 of 100 took 11.774s\n",
      "  training loss (in-iteration): \t0.087842\n",
      "  validation loss: \t\t\t11.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.60it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 of 100 took 11.598s\n",
      "  training loss (in-iteration): \t0.088752\n",
      "  validation loss: \t\t\t8.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.55it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 of 100 took 12.179s\n",
      "  training loss (in-iteration): \t0.089361\n",
      "  validation loss: \t\t\t8.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.58it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 of 100 took 11.875s\n",
      "  training loss (in-iteration): \t0.085916\n",
      "  validation loss: \t\t\t9.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.54it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 of 100 took 11.602s\n",
      "  training loss (in-iteration): \t0.085072\n",
      "  validation loss: \t\t\t9.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.93it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 of 100 took 11.736s\n",
      "  training loss (in-iteration): \t0.083357\n",
      "  validation loss: \t\t\t8.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.72it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 of 100 took 11.904s\n",
      "  training loss (in-iteration): \t0.085056\n",
      "  validation loss: \t\t\t9.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 45.94it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 of 100 took 12.346s\n",
      "  training loss (in-iteration): \t0.086923\n",
      "  validation loss: \t\t\t10.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.70it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 80.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 of 100 took 11.434s\n",
      "  training loss (in-iteration): \t0.089247\n",
      "  validation loss: \t\t\t9.140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.00it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 of 100 took 11.411s\n",
      "  training loss (in-iteration): \t0.087006\n",
      "  validation loss: \t\t\t8.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 57.14it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 of 100 took 11.535s\n",
      "  training loss (in-iteration): \t0.087975\n",
      "  validation loss: \t\t\t8.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.29it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 of 100 took 11.948s\n",
      "  training loss (in-iteration): \t0.086843\n",
      "  validation loss: \t\t\t8.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.49it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 of 100 took 11.797s\n",
      "  training loss (in-iteration): \t0.088605\n",
      "  validation loss: \t\t\t8.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.85it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 of 100 took 11.837s\n",
      "  training loss (in-iteration): \t0.086585\n",
      "  validation loss: \t\t\t7.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.72it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 of 100 took 11.640s\n",
      "  training loss (in-iteration): \t0.086180\n",
      "  validation loss: \t\t\t8.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.92it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 of 100 took 12.013s\n",
      "  training loss (in-iteration): \t0.085119\n",
      "  validation loss: \t\t\t8.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.18it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 of 100 took 11.757s\n",
      "  training loss (in-iteration): \t0.085787\n",
      "  validation loss: \t\t\t7.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 80.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 of 100 took 11.311s\n",
      "  training loss (in-iteration): \t0.088237\n",
      "  validation loss: \t\t\t8.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.82it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 of 100 took 11.498s\n",
      "  training loss (in-iteration): \t0.085002\n",
      "  validation loss: \t\t\t12.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.56it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 of 100 took 11.654s\n",
      "  training loss (in-iteration): \t0.083907\n",
      "  validation loss: \t\t\t8.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 80.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 of 100 took 11.362s\n",
      "  training loss (in-iteration): \t0.076285\n",
      "  validation loss: \t\t\t9.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.89it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 of 100 took 11.548s\n",
      "  training loss (in-iteration): \t0.077118\n",
      "  validation loss: \t\t\t7.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.86it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 of 100 took 11.511s\n",
      "  training loss (in-iteration): \t0.077531\n",
      "  validation loss: \t\t\t11.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.89it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 of 100 took 12.106s\n",
      "  training loss (in-iteration): \t0.082357\n",
      "  validation loss: \t\t\t7.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.62it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 of 100 took 11.966s\n",
      "  training loss (in-iteration): \t0.085196\n",
      "  validation loss: \t\t\t7.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.61it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 of 100 took 11.528s\n",
      "  training loss (in-iteration): \t0.082730\n",
      "  validation loss: \t\t\t10.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.35it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 of 100 took 11.729s\n",
      "  training loss (in-iteration): \t0.080870\n",
      "  validation loss: \t\t\t10.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.35it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 of 100 took 11.703s\n",
      "  training loss (in-iteration): \t0.078609\n",
      "  validation loss: \t\t\t8.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.39it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 of 100 took 11.688s\n",
      "  training loss (in-iteration): \t0.085666\n",
      "  validation loss: \t\t\t7.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 57.49it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 of 100 took 11.463s\n",
      "  training loss (in-iteration): \t0.089256\n",
      "  validation loss: \t\t\t9.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.97it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 of 100 took 11.527s\n",
      "  training loss (in-iteration): \t0.088514\n",
      "  validation loss: \t\t\t7.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.89it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 100 took 11.842s\n",
      "  training loss (in-iteration): \t0.084713\n",
      "  validation loss: \t\t\t8.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.84it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 of 100 took 11.773s\n",
      "  training loss (in-iteration): \t0.082132\n",
      "  validation loss: \t\t\t7.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.23it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 of 100 took 11.853s\n",
      "  training loss (in-iteration): \t0.081602\n",
      "  validation loss: \t\t\t7.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.95it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 of 100 took 11.929s\n",
      "  training loss (in-iteration): \t0.082343\n",
      "  validation loss: \t\t\t8.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.09it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 of 100 took 12.042s\n",
      "  training loss (in-iteration): \t0.081952\n",
      "  validation loss: \t\t\t8.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.19it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 76.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 of 100 took 11.900s\n",
      "  training loss (in-iteration): \t0.086799\n",
      "  validation loss: \t\t\t9.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.56it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 of 100 took 11.413s\n",
      "  training loss (in-iteration): \t0.089416\n",
      "  validation loss: \t\t\t7.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.25it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 of 100 took 11.631s\n",
      "  training loss (in-iteration): \t0.090417\n",
      "  validation loss: \t\t\t7.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.74it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 72.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 of 100 took 12.511s\n",
      "  training loss (in-iteration): \t0.083591\n",
      "  validation loss: \t\t\t7.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.44it/s]\n",
      "100%|██████████| 765/765 [00:10<00:00, 75.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 of 100 took 12.230s\n",
      "  training loss (in-iteration): \t0.076509\n",
      "  validation loss: \t\t\t7.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.47it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 77.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 of 100 took 11.760s\n",
      "  training loss (in-iteration): \t0.074634\n",
      "  validation loss: \t\t\t7.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 53.85it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 76.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 of 100 took 11.820s\n",
      "  training loss (in-iteration): \t0.075403\n",
      "  validation loss: \t\t\t7.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.36it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 of 100 took 11.749s\n",
      "  training loss (in-iteration): \t0.075248\n",
      "  validation loss: \t\t\t9.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.39it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 of 100 took 11.537s\n",
      "  training loss (in-iteration): \t0.076102\n",
      "  validation loss: \t\t\t7.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.77it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 of 100 took 11.467s\n",
      "  training loss (in-iteration): \t0.076962\n",
      "  validation loss: \t\t\t8.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.97it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 79.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 of 100 took 11.631s\n",
      "  training loss (in-iteration): \t0.076662\n",
      "  validation loss: \t\t\t7.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.51it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 of 100 took 11.642s\n",
      "  training loss (in-iteration): \t0.076131\n",
      "  validation loss: \t\t\t7.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.67it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 of 100 took 11.556s\n",
      "  training loss (in-iteration): \t0.073978\n",
      "  validation loss: \t\t\t18.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.59it/s]\n",
      "100%|██████████| 765/765 [00:09<00:00, 78.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 of 100 took 11.550s\n",
      "  training loss (in-iteration): \t0.078473\n",
      "  validation loss: \t\t\t7.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_emb = FullNetworkEmb().to(device)\n",
    "opt = torch.optim.Adam(model_emb.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "train(model_emb, opt, batch_size=batch_size, patience=5, num_epochs=num_epochs, model_name='model_emb', batches_per_epoch = 100, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:09<00:00, 81.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.07374\n",
      "\tMAE:\t2597.32120\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_eval(model_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dropout + BatchNorm + Conv + Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TitleEncoderEmbV1(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "        self.emb.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32)\n",
    "        )\n",
    "        self.emb.weight.requires_grad=False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(300, 100, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.05)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(100, out_size, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_size)\n",
    "        \n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        h = self.pool1(h)\n",
    "        h = self.dp(h)\n",
    "        h = self.dense(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DescriptionEncoderEmbV1(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "        self.emb.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32)\n",
    "        )\n",
    "        self.emb.weight.requires_grad=False\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(300, 100, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.05)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(100, out_size, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_size)\n",
    "        \n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "\n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        h = self.pool1(h)\n",
    "        h = self.dp(h)\n",
    "        h = self.dense(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetworkEmbV1(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoderEmbV1(n_tokens, out_size=64)\n",
    "        self.desc_encoder = DescriptionEncoderEmbV1(n_tokens, out_size=64)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        self.dense1 = nn.Linear(n_cat_features, 300)\n",
    "        self.dp = nn.Dropout(0.05)\n",
    "        self.dense2 = nn.Linear(300, 64)\n",
    "        self.dense3 = nn.Linear(192, 96)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        self.dense4 = nn.Linear(96, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.dense1(cat_features)\n",
    "        cat_h = self.dense2(cat_h)\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        \n",
    "        finale = self.dense3(joint_h)\n",
    "        finale = self.dense4(finale)\n",
    "\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return finale[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.46it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 16.008s\n",
      "  training loss (in-iteration): \t3.153548\n",
      "  validation loss: \t\t\t22.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.84it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 100 took 16.173s\n",
      "  training loss (in-iteration): \t1.765717\n",
      "  validation loss: \t\t\t16.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.18it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 100 took 16.082s\n",
      "  training loss (in-iteration): \t1.270349\n",
      "  validation loss: \t\t\t15.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.59it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 100 took 15.185s\n",
      "  training loss (in-iteration): \t0.330627\n",
      "  validation loss: \t\t\t27.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.00it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 100 took 15.159s\n",
      "  training loss (in-iteration): \t0.299533\n",
      "  validation loss: \t\t\t28.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.83it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 100 took 15.284s\n",
      "  training loss (in-iteration): \t0.298808\n",
      "  validation loss: \t\t\t15.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.24it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 100 took 16.146s\n",
      "  training loss (in-iteration): \t0.274751\n",
      "  validation loss: \t\t\t13.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.16it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 100 took 15.226s\n",
      "  training loss (in-iteration): \t0.246238\n",
      "  validation loss: \t\t\t27.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.63it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 100 took 15.433s\n",
      "  training loss (in-iteration): \t0.228566\n",
      "  validation loss: \t\t\t12.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.23it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 100 took 15.420s\n",
      "  training loss (in-iteration): \t0.210342\n",
      "  validation loss: \t\t\t12.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.75it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 100 took 15.324s\n",
      "  training loss (in-iteration): \t0.210467\n",
      "  validation loss: \t\t\t35.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.09it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 100 took 15.500s\n",
      "  training loss (in-iteration): \t0.213396\n",
      "  validation loss: \t\t\t12.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.42it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 100 took 15.405s\n",
      "  training loss (in-iteration): \t0.264718\n",
      "  validation loss: \t\t\t26.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.97it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 100 took 15.330s\n",
      "  training loss (in-iteration): \t0.273193\n",
      "  validation loss: \t\t\t32.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.29it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 100 took 15.473s\n",
      "  training loss (in-iteration): \t0.267771\n",
      "  validation loss: \t\t\t10.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.42it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 100 took 15.247s\n",
      "  training loss (in-iteration): \t0.221937\n",
      "  validation loss: \t\t\t10.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.41it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 100 took 15.197s\n",
      "  training loss (in-iteration): \t0.189044\n",
      "  validation loss: \t\t\t12.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.34it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 100 took 15.229s\n",
      "  training loss (in-iteration): \t0.174924\n",
      "  validation loss: \t\t\t20.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.59it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 100 took 15.237s\n",
      "  training loss (in-iteration): \t0.171435\n",
      "  validation loss: \t\t\t10.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.36it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 100 took 15.268s\n",
      "  training loss (in-iteration): \t0.176440\n",
      "  validation loss: \t\t\t12.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.40it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 100 took 15.266s\n",
      "  training loss (in-iteration): \t0.173478\n",
      "  validation loss: \t\t\t16.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.12it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 100 took 15.392s\n",
      "  training loss (in-iteration): \t0.173534\n",
      "  validation loss: \t\t\t12.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.38it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 100 took 15.249s\n",
      "  training loss (in-iteration): \t0.170813\n",
      "  validation loss: \t\t\t11.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.34it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 100 took 15.393s\n",
      "  training loss (in-iteration): \t0.166143\n",
      "  validation loss: \t\t\t11.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.06it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 100 took 15.275s\n",
      "  training loss (in-iteration): \t0.166549\n",
      "  validation loss: \t\t\t11.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.17it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 100 took 15.504s\n",
      "  training loss (in-iteration): \t0.170713\n",
      "  validation loss: \t\t\t10.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.83it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 100 took 15.186s\n",
      "  training loss (in-iteration): \t0.171217\n",
      "  validation loss: \t\t\t12.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.30it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 100 took 15.235s\n",
      "  training loss (in-iteration): \t0.184694\n",
      "  validation loss: \t\t\t13.363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.18it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 100 took 15.493s\n",
      "  training loss (in-iteration): \t0.191511\n",
      "  validation loss: \t\t\t10.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.64it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 100 took 15.440s\n",
      "  training loss (in-iteration): \t0.186567\n",
      "  validation loss: \t\t\t10.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.01it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 100 took 15.512s\n",
      "  training loss (in-iteration): \t0.165789\n",
      "  validation loss: \t\t\t9.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.95it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 100 took 15.430s\n",
      "  training loss (in-iteration): \t0.155160\n",
      "  validation loss: \t\t\t10.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.42it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 100 took 15.209s\n",
      "  training loss (in-iteration): \t0.162961\n",
      "  validation loss: \t\t\t19.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.91it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 100 took 15.316s\n",
      "  training loss (in-iteration): \t0.173643\n",
      "  validation loss: \t\t\t10.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.69it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 100 took 15.256s\n",
      "  training loss (in-iteration): \t0.174134\n",
      "  validation loss: \t\t\t24.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.28it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 100 took 15.169s\n",
      "  training loss (in-iteration): \t0.179141\n",
      "  validation loss: \t\t\t18.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.11it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 100 took 15.302s\n",
      "  training loss (in-iteration): \t0.187271\n",
      "  validation loss: \t\t\t11.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.19it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 100 took 15.194s\n",
      "  training loss (in-iteration): \t0.184889\n",
      "  validation loss: \t\t\t10.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.20it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 of 100 took 15.377s\n",
      "  training loss (in-iteration): \t0.178101\n",
      "  validation loss: \t\t\t18.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.87it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 of 100 took 15.143s\n",
      "  training loss (in-iteration): \t0.157260\n",
      "  validation loss: \t\t\t10.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.14it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 100 took 15.292s\n",
      "  training loss (in-iteration): \t0.158496\n",
      "  validation loss: \t\t\t20.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.34it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 of 100 took 15.453s\n",
      "  training loss (in-iteration): \t0.156441\n",
      "  validation loss: \t\t\t9.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.23it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 of 100 took 15.515s\n",
      "  training loss (in-iteration): \t0.158672\n",
      "  validation loss: \t\t\t9.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.53it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 100 took 15.197s\n",
      "  training loss (in-iteration): \t0.154243\n",
      "  validation loss: \t\t\t10.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.26it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 of 100 took 15.281s\n",
      "  training loss (in-iteration): \t0.156003\n",
      "  validation loss: \t\t\t14.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.79it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 of 100 took 15.380s\n",
      "  training loss (in-iteration): \t0.150516\n",
      "  validation loss: \t\t\t9.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.91it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 of 100 took 15.234s\n",
      "  training loss (in-iteration): \t0.147933\n",
      "  validation loss: \t\t\t9.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.43it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 100 took 15.264s\n",
      "  training loss (in-iteration): \t0.137702\n",
      "  validation loss: \t\t\t13.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.38it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 of 100 took 15.197s\n",
      "  training loss (in-iteration): \t0.133582\n",
      "  validation loss: \t\t\t19.123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.81it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 of 100 took 15.296s\n",
      "  training loss (in-iteration): \t0.140006\n",
      "  validation loss: \t\t\t10.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.03it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 of 100 took 15.503s\n",
      "  training loss (in-iteration): \t0.141728\n",
      "  validation loss: \t\t\t9.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.27it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 of 100 took 15.438s\n",
      "  training loss (in-iteration): \t0.145934\n",
      "  validation loss: \t\t\t9.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.18it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 of 100 took 15.321s\n",
      "  training loss (in-iteration): \t0.135997\n",
      "  validation loss: \t\t\t11.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.43it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 of 100 took 15.125s\n",
      "  training loss (in-iteration): \t0.142480\n",
      "  validation loss: \t\t\t9.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.65it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 of 100 took 15.279s\n",
      "  training loss (in-iteration): \t0.145728\n",
      "  validation loss: \t\t\t14.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.56it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 of 100 took 15.306s\n",
      "  training loss (in-iteration): \t0.150332\n",
      "  validation loss: \t\t\t17.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.52it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 of 100 took 15.201s\n",
      "  training loss (in-iteration): \t0.150371\n",
      "  validation loss: \t\t\t10.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.12it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 of 100 took 15.191s\n",
      "  training loss (in-iteration): \t0.140059\n",
      "  validation loss: \t\t\t12.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.75it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 of 100 took 15.384s\n",
      "  training loss (in-iteration): \t0.145516\n",
      "  validation loss: \t\t\t19.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.11it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 of 100 took 15.384s\n",
      "  training loss (in-iteration): \t0.133469\n",
      "  validation loss: \t\t\t9.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.83it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 of 100 took 15.293s\n",
      "  training loss (in-iteration): \t0.146128\n",
      "  validation loss: \t\t\t10.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.52it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 of 100 took 15.195s\n",
      "  training loss (in-iteration): \t0.159752\n",
      "  validation loss: \t\t\t14.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.92it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 of 100 took 15.335s\n",
      "  training loss (in-iteration): \t0.150114\n",
      "  validation loss: \t\t\t9.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.51it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 of 100 took 15.332s\n",
      "  training loss (in-iteration): \t0.140628\n",
      "  validation loss: \t\t\t11.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.54it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 of 100 took 15.457s\n",
      "  training loss (in-iteration): \t0.118402\n",
      "  validation loss: \t\t\t9.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.68it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 of 100 took 15.137s\n",
      "  training loss (in-iteration): \t0.125809\n",
      "  validation loss: \t\t\t10.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.07it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 of 100 took 15.387s\n",
      "  training loss (in-iteration): \t0.130766\n",
      "  validation loss: \t\t\t10.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.23it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 of 100 took 15.197s\n",
      "  training loss (in-iteration): \t0.140950\n",
      "  validation loss: \t\t\t9.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.37it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 of 100 took 15.285s\n",
      "  training loss (in-iteration): \t0.141839\n",
      "  validation loss: \t\t\t10.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.31it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 of 100 took 15.329s\n",
      "  training loss (in-iteration): \t0.153509\n",
      "  validation loss: \t\t\t13.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.46it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 of 100 took 15.264s\n",
      "  training loss (in-iteration): \t0.144937\n",
      "  validation loss: \t\t\t9.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.26it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 of 100 took 15.216s\n",
      "  training loss (in-iteration): \t0.138978\n",
      "  validation loss: \t\t\t9.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.97it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 of 100 took 15.150s\n",
      "  training loss (in-iteration): \t0.121958\n",
      "  validation loss: \t\t\t15.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.87it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 of 100 took 15.193s\n",
      "  training loss (in-iteration): \t0.125849\n",
      "  validation loss: \t\t\t9.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.76it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 of 100 took 15.236s\n",
      "  training loss (in-iteration): \t0.130841\n",
      "  validation loss: \t\t\t31.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.47it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 of 100 took 15.228s\n",
      "  training loss (in-iteration): \t0.136431\n",
      "  validation loss: \t\t\t9.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.57it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 of 100 took 15.276s\n",
      "  training loss (in-iteration): \t0.133650\n",
      "  validation loss: \t\t\t9.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.58it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 of 100 took 15.273s\n",
      "  training loss (in-iteration): \t0.137628\n",
      "  validation loss: \t\t\t9.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.60it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 of 100 took 15.397s\n",
      "  training loss (in-iteration): \t0.127345\n",
      "  validation loss: \t\t\t9.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.20it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 of 100 took 15.343s\n",
      "  training loss (in-iteration): \t0.130899\n",
      "  validation loss: \t\t\t10.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.21it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 of 100 took 15.290s\n",
      "  training loss (in-iteration): \t0.122929\n",
      "  validation loss: \t\t\t12.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.34it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 100 took 15.195s\n",
      "  training loss (in-iteration): \t0.120933\n",
      "  validation loss: \t\t\t9.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.75it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 of 100 took 15.243s\n",
      "  training loss (in-iteration): \t0.120651\n",
      "  validation loss: \t\t\t10.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.77it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 of 100 took 15.215s\n",
      "  training loss (in-iteration): \t0.125162\n",
      "  validation loss: \t\t\t11.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.84it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 of 100 took 15.216s\n",
      "  training loss (in-iteration): \t0.133325\n",
      "  validation loss: \t\t\t9.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.58it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 of 100 took 15.152s\n",
      "  training loss (in-iteration): \t0.125970\n",
      "  validation loss: \t\t\t9.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.76it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 of 100 took 15.299s\n",
      "  training loss (in-iteration): \t0.136586\n",
      "  validation loss: \t\t\t15.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.60it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 of 100 took 15.203s\n",
      "  training loss (in-iteration): \t0.132240\n",
      "  validation loss: \t\t\t10.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.27it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 of 100 took 15.297s\n",
      "  training loss (in-iteration): \t0.133180\n",
      "  validation loss: \t\t\t9.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.52it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 of 100 took 15.235s\n",
      "  training loss (in-iteration): \t0.119396\n",
      "  validation loss: \t\t\t10.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.55it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 of 100 took 15.310s\n",
      "  training loss (in-iteration): \t0.116319\n",
      "  validation loss: \t\t\t10.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.07it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 of 100 took 15.226s\n",
      "  training loss (in-iteration): \t0.113876\n",
      "  validation loss: \t\t\t11.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.40it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 of 100 took 15.236s\n",
      "  training loss (in-iteration): \t0.123505\n",
      "  validation loss: \t\t\t14.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.58it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 of 100 took 15.222s\n",
      "  training loss (in-iteration): \t0.127981\n",
      "  validation loss: \t\t\t10.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.06it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 of 100 took 15.220s\n",
      "  training loss (in-iteration): \t0.131830\n",
      "  validation loss: \t\t\t10.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.78it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 of 100 took 15.236s\n",
      "  training loss (in-iteration): \t0.123183\n",
      "  validation loss: \t\t\t12.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.75it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 of 100 took 15.268s\n",
      "  training loss (in-iteration): \t0.114458\n",
      "  validation loss: \t\t\t9.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.46it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 of 100 took 15.261s\n",
      "  training loss (in-iteration): \t0.115530\n",
      "  validation loss: \t\t\t10.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.90it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 61.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 of 100 took 15.160s\n",
      "  training loss (in-iteration): \t0.115026\n",
      "  validation loss: \t\t\t9.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.11it/s]\n",
      "100%|██████████| 765/765 [00:12<00:00, 60.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 of 100 took 15.354s\n",
      "  training loss (in-iteration): \t0.117176\n",
      "  validation loss: \t\t\t10.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_emb_v1 = FullNetworkEmbV1().to(device)\n",
    "opt = torch.optim.Adam(model_emb_v1.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "train(model_emb_v1, opt, batch_size=batch_size, patience=5, num_epochs=num_epochs, model_name='model_emb_v1', batches_per_epoch = 100, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:12<00:00, 60.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.10341\n",
      "\tMAE:\t3040.41838\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_eval(model_emb_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.2: Actually make it work\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__.\n",
    "\n",
    "Try __at least 3 options__ from the list below for a passing grade. If you're into \n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm1d`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to do max pooling:\n",
    "* Max over time - our `GlobalMaxPooling`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a small neural network\n",
    "\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$\n",
    "\n",
    "#### C) Fun with embeddings\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained word2vec from [here](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/).\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "  * Please bear in mind that while convolution uses [batch, units, time] dim order, \n",
    "    recurrent units are built for [batch, time, unit]. You may need to `torch.transpose`.\n",
    "\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [keras](https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L461) for inspiration.\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.state_dict`\n",
    "  * Plotting learning curves is usually a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "Добавил Early Stopping, но ни в одном из тренировочных процессов он не успел сработать. Также созданы две модели: первая, model_emb, использует Google's trained Word2Vec model, а вторая, model_emb_1, также имеет несколько дополнительных слоев, в их числе BatchNorm и Dropout слои.\n",
    "\n",
    "Первая модель улучшила результат baseline модели. Вторая себя проявила хуже, но также улучшила результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}