{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cuda_availability = torch.cuda.is_available()\n",
    "if cuda_availability:\n",
    "    device = torch.device('cuda:{}'.format(torch.cuda.current_device()))\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetFromSubset(Dataset):\n",
    "    \"\"\"\n",
    "    https://discuss.pytorch.org/t/torch-utils-data-dataset-random-split/32209/3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_val_data = CIFAR10(\"./cifar_data/\", train=True)\n",
    "\n",
    "val_part = int(0.1 * len(train_val_data))\n",
    "train_part = len(train_val_data) - val_part\n",
    "\n",
    "train_subset, val_subset = random_split(train_val_data, [train_part, val_part])\n",
    "\n",
    "train_loader = DatasetFromSubset(train_subset, transform=transform_augment)\n",
    "val_loader = DatasetFromSubset(val_subset, transform=transform_test)\n",
    "test_loader = CIFAR10(\"./cifar_data/\", train=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, X_batch, y_batch):\n",
    "    X_batch = torch.as_tensor(X_batch, dtype=torch.float32, device=device)\n",
    "    y_batch = torch.as_tensor(y_batch, dtype=torch.int64, device=device)\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "    num_workers = 0 if device == 'cpu' else 4\n",
    "    pin_memory = False\n",
    "    \n",
    "    test_batch_gen = torch.utils.data.DataLoader(test_loader,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers,\n",
    "                                             pin_memory=pin_memory)\n",
    "    \n",
    "    model.train(False)\n",
    "    test_batch_acc = []\n",
    "    for X_batch, y_batch in test_batch_gen:\n",
    "        X_batch = X_batch.to(device, non_blocking=True)\n",
    "        logits = model(X_batch)\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        test_batch_acc.append(np.mean(y_batch.cpu().numpy() == y_pred))\n",
    "\n",
    "    test_accuracy = np.mean(test_batch_acc)\n",
    "\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_accuracy * 100))\n",
    "\n",
    "    if test_accuracy * 100 > 95:\n",
    "        print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "    elif test_accuracy * 100 > 90:\n",
    "        print(\"U'r freakin' amazin'!\")\n",
    "    elif test_accuracy * 100 > 80:\n",
    "        print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "    elif test_accuracy * 100 > 70:\n",
    "        print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "    elif test_accuracy * 100 > 60:\n",
    "        print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "    elif test_accuracy * 100 > 50:\n",
    "        print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "    else:\n",
    "        print(\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, opt, sch, batch_size=512, patience=10, num_epochs=10, model_name='simple_model', save=False):\n",
    "    train_loss_ar = []\n",
    "    val_accuracy_ar = []\n",
    "\n",
    "    trigger_times = 0\n",
    "\n",
    "    prev_val_accuracy = -np.Inf\n",
    "    val_accuracy_max = -np.Inf\n",
    "\n",
    "    num_workers = 0 if device == 'cpu' else 4\n",
    "    pin_memory = False\n",
    "\n",
    "    train_batch_gen = torch.utils.data.DataLoader(train_loader,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=num_workers,\n",
    "                                                  pin_memory=pin_memory)\n",
    "\n",
    "    val_batch_gen = torch.utils.data.DataLoader(val_loader,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)\n",
    "        for X_batch, y_batch in train_batch_gen:\n",
    "            X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
    "            loss = compute_loss(model, X_batch, y_batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sch.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            train_loss_ar.append(loss.item())\n",
    "\n",
    "        model.train(False)\n",
    "        for X_batch, y_batch in val_batch_gen:\n",
    "            X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
    "            logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
    "            y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "            val_accuracy_ar.append(np.mean((y_batch.cpu().numpy() == y_pred)))\n",
    "\n",
    "        val_accuracy = np.mean(val_accuracy_ar[-len(val_loader) // batch_size:])\n",
    "\n",
    "        if val_accuracy > val_accuracy_max and save:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'valid_accuracy': val_accuracy,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': opt.state_dict(),\n",
    "            }, 'models/' + model_name + f'_{epoch+1}.pth')\n",
    "\n",
    "            val_accuracy_max = val_accuracy\n",
    "\n",
    "        if val_accuracy < prev_val_accuracy:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping\\n')\n",
    "                get_accuracy(model)\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "        prev_val_accuracy = val_accuracy\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss_ar[-len(train_loader) // batch_size:])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "            val_accuracy * 100))\n",
    "        print(\"  learning rate: \\t\\t\\t{:.4f}\".format(\n",
    "            sch.get_last_lr()[0]))\n",
    "\n",
    "    get_accuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=(5, 5), bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(kernel_size=(3, 3)),\n",
    "    nn.GELU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=(5, 5), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.GELU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(1600, 64),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(64, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 512\n",
    "patience = 5\n",
    "steps_per_epoch = len(train_loader)//batch_size + 1 if len(train_loader)%batch_size != 0 else len(train_loader)//batch_size\n",
    "\n",
    "optimizer = torch.optim.AdamW(simple_model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epoch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 7.988s\n",
      "  training loss (in-iteration): \t0.942151\n",
      "  validation accuracy: \t\t\t68.96 %\n",
      "  learning rate: \t\t\t0.0028 %\n",
      "Epoch 2 of 10 took 7.712s\n",
      "  training loss (in-iteration): \t1.017422\n",
      "  validation accuracy: \t\t\t66.21 %\n",
      "  learning rate: \t\t\t0.0076 %\n",
      "Epoch 3 of 10 took 7.843s\n",
      "  training loss (in-iteration): \t1.094179\n",
      "  validation accuracy: \t\t\t64.32 %\n",
      "  learning rate: \t\t\t0.0100 %\n",
      "Epoch 4 of 10 took 7.566s\n",
      "  training loss (in-iteration): \t1.072090\n",
      "  validation accuracy: \t\t\t66.85 %\n",
      "  learning rate: \t\t\t0.0095 %\n",
      "Epoch 5 of 10 took 8.052s\n",
      "  training loss (in-iteration): \t1.038102\n",
      "  validation accuracy: \t\t\t67.48 %\n",
      "  learning rate: \t\t\t0.0081 %\n",
      "Epoch 6 of 10 took 7.636s\n",
      "  training loss (in-iteration): \t0.990658\n",
      "  validation accuracy: \t\t\t70.42 %\n",
      "  learning rate: \t\t\t0.0061 %\n",
      "Epoch 7 of 10 took 7.746s\n",
      "  training loss (in-iteration): \t0.942064\n",
      "  validation accuracy: \t\t\t72.13 %\n",
      "  learning rate: \t\t\t0.0039 %\n",
      "Epoch 8 of 10 took 7.677s\n",
      "  training loss (in-iteration): \t0.902883\n",
      "  validation accuracy: \t\t\t73.39 %\n",
      "  learning rate: \t\t\t0.0019 %\n",
      "Epoch 9 of 10 took 7.662s\n",
      "  training loss (in-iteration): \t0.865035\n",
      "  validation accuracy: \t\t\t74.20 %\n",
      "  learning rate: \t\t\t0.0005 %\n",
      "Epoch 10 of 10 took 7.711s\n",
      "  training loss (in-iteration): \t0.843758\n",
      "  validation accuracy: \t\t\t74.62 %\n",
      "  learning rate: \t\t\t0.0000 %\n",
      "Final results:\n",
      "  test accuracy:\t\t73.84 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "train(simple_model, optimizer, sch=scheduler, batch_size=batch_size, patience=patience, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CNN v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding, activation, dropout, batch_norm, pool=None, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.out_channels = out_dim\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.batch_norm_2d = nn.BatchNorm2d(out_dim)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=(kernel_size, kernel_size), padding=padding, dilation=dilation)\n",
    "\n",
    "        if pool == 'max':\n",
    "            self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        elif pool == 'mean':\n",
    "            self.pool = nn.AvgPool2d(2)\n",
    "        elif pool is None:\n",
    "            self.pool = pool\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm_2d(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = nn.modules.activation.GELU()\n",
    "        self.dropout = 0.05\n",
    "        self.batch_norm = True\n",
    "\n",
    "        self.conv1 = ConvBlock(3, 64, 7, 3, self.activation, self.dropout, self.batch_norm)\n",
    "        self.conv2 = ConvBlock(64, 64, 3, 1, self.activation, self.dropout, self.batch_norm, pool='max')\n",
    "        self.conv3 = ConvBlock(64, 128, 3, 1, self.activation, self.dropout, self.batch_norm)\n",
    "        self.conv4 = ConvBlock(128, 128, 3, 1, self.activation, self.dropout, self.batch_norm, pool='max')\n",
    "        self.conv5 = ConvBlock(128, 256, 3, 1, self.activation, self.dropout, self.batch_norm)\n",
    "        self.conv6 = ConvBlock(256, 256, 3, 1, self.activation, self.dropout, self.batch_norm, pool='max')\n",
    "        self.conv7 = ConvBlock(256, 512, 3, 1, self.activation, self.dropout, self.batch_norm)\n",
    "        self.conv8 = ConvBlock(512, 512, 3, 1, self.activation, self.dropout, self.batch_norm, pool='max')\n",
    "\n",
    "        self.fc = nn.Linear(2048 , 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 16.631s\n",
      "  training loss (in-iteration): \t1.677367\n",
      "  validation accuracy: \t\t\t46.05 %\n",
      "  learning rate: \t\t\t0.0004\n",
      "Epoch 2 of 100 took 16.643s\n",
      "  training loss (in-iteration): \t1.279906\n",
      "  validation accuracy: \t\t\t60.00 %\n",
      "  learning rate: \t\t\t0.0005\n",
      "Epoch 3 of 100 took 16.932s\n",
      "  training loss (in-iteration): \t1.090327\n",
      "  validation accuracy: \t\t\t67.76 %\n",
      "  learning rate: \t\t\t0.0006\n",
      "Epoch 4 of 100 took 16.983s\n",
      "  training loss (in-iteration): \t0.975070\n",
      "  validation accuracy: \t\t\t67.92 %\n",
      "  learning rate: \t\t\t0.0008\n",
      "Epoch 5 of 100 took 17.140s\n",
      "  training loss (in-iteration): \t0.883390\n",
      "  validation accuracy: \t\t\t73.33 %\n",
      "  learning rate: \t\t\t0.0010\n",
      "Epoch 6 of 100 took 17.336s\n",
      "  training loss (in-iteration): \t0.814127\n",
      "  validation accuracy: \t\t\t76.65 %\n",
      "  learning rate: \t\t\t0.0013\n",
      "Epoch 7 of 100 took 17.548s\n",
      "  training loss (in-iteration): \t0.769796\n",
      "  validation accuracy: \t\t\t78.36 %\n",
      "  learning rate: \t\t\t0.0016\n",
      "Epoch 8 of 100 took 17.399s\n",
      "  training loss (in-iteration): \t0.726065\n",
      "  validation accuracy: \t\t\t77.02 %\n",
      "  learning rate: \t\t\t0.0020\n",
      "Epoch 9 of 100 took 17.532s\n",
      "  training loss (in-iteration): \t0.702387\n",
      "  validation accuracy: \t\t\t78.11 %\n",
      "  learning rate: \t\t\t0.0024\n",
      "Epoch 10 of 100 took 18.049s\n",
      "  training loss (in-iteration): \t0.657886\n",
      "  validation accuracy: \t\t\t79.48 %\n",
      "  learning rate: \t\t\t0.0028\n",
      "Epoch 11 of 100 took 18.086s\n",
      "  training loss (in-iteration): \t0.653842\n",
      "  validation accuracy: \t\t\t81.45 %\n",
      "  learning rate: \t\t\t0.0032\n",
      "Epoch 12 of 100 took 17.712s\n",
      "  training loss (in-iteration): \t0.614034\n",
      "  validation accuracy: \t\t\t78.83 %\n",
      "  learning rate: \t\t\t0.0037\n",
      "Epoch 13 of 100 took 17.766s\n",
      "  training loss (in-iteration): \t0.588428\n",
      "  validation accuracy: \t\t\t80.55 %\n",
      "  learning rate: \t\t\t0.0042\n",
      "Epoch 14 of 100 took 17.752s\n",
      "  training loss (in-iteration): \t0.567173\n",
      "  validation accuracy: \t\t\t81.98 %\n",
      "  learning rate: \t\t\t0.0047\n",
      "Epoch 15 of 100 took 17.944s\n",
      "  training loss (in-iteration): \t0.541521\n",
      "  validation accuracy: \t\t\t82.92 %\n",
      "  learning rate: \t\t\t0.0052\n",
      "Epoch 16 of 100 took 17.969s\n",
      "  training loss (in-iteration): \t0.528044\n",
      "  validation accuracy: \t\t\t83.17 %\n",
      "  learning rate: \t\t\t0.0057\n",
      "Epoch 17 of 100 took 17.991s\n",
      "  training loss (in-iteration): \t0.507204\n",
      "  validation accuracy: \t\t\t83.74 %\n",
      "  learning rate: \t\t\t0.0062\n",
      "Epoch 18 of 100 took 18.036s\n",
      "  training loss (in-iteration): \t0.482067\n",
      "  validation accuracy: \t\t\t83.89 %\n",
      "  learning rate: \t\t\t0.0067\n",
      "Epoch 19 of 100 took 17.992s\n",
      "  training loss (in-iteration): \t0.473955\n",
      "  validation accuracy: \t\t\t84.15 %\n",
      "  learning rate: \t\t\t0.0072\n",
      "Epoch 20 of 100 took 17.948s\n",
      "  training loss (in-iteration): \t0.457069\n",
      "  validation accuracy: \t\t\t85.81 %\n",
      "  learning rate: \t\t\t0.0076\n",
      "Epoch 21 of 100 took 17.853s\n",
      "  training loss (in-iteration): \t0.453766\n",
      "  validation accuracy: \t\t\t84.75 %\n",
      "  learning rate: \t\t\t0.0080\n",
      "Epoch 22 of 100 took 17.859s\n",
      "  training loss (in-iteration): \t0.435650\n",
      "  validation accuracy: \t\t\t85.63 %\n",
      "  learning rate: \t\t\t0.0084\n",
      "Epoch 23 of 100 took 17.881s\n",
      "  training loss (in-iteration): \t0.425444\n",
      "  validation accuracy: \t\t\t85.63 %\n",
      "  learning rate: \t\t\t0.0088\n",
      "Epoch 24 of 100 took 17.989s\n",
      "  training loss (in-iteration): \t0.409988\n",
      "  validation accuracy: \t\t\t84.88 %\n",
      "  learning rate: \t\t\t0.0091\n",
      "Epoch 25 of 100 took 17.970s\n",
      "  training loss (in-iteration): \t0.400097\n",
      "  validation accuracy: \t\t\t86.25 %\n",
      "  learning rate: \t\t\t0.0094\n",
      "Epoch 26 of 100 took 17.964s\n",
      "  training loss (in-iteration): \t0.390876\n",
      "  validation accuracy: \t\t\t85.72 %\n",
      "  learning rate: \t\t\t0.0096\n",
      "Epoch 27 of 100 took 18.004s\n",
      "  training loss (in-iteration): \t0.379759\n",
      "  validation accuracy: \t\t\t86.77 %\n",
      "  learning rate: \t\t\t0.0098\n",
      "Epoch 28 of 100 took 17.995s\n",
      "  training loss (in-iteration): \t0.369622\n",
      "  validation accuracy: \t\t\t86.47 %\n",
      "  learning rate: \t\t\t0.0099\n",
      "Epoch 29 of 100 took 17.909s\n",
      "  training loss (in-iteration): \t0.358797\n",
      "  validation accuracy: \t\t\t86.46 %\n",
      "  learning rate: \t\t\t0.0100\n",
      "Epoch 30 of 100 took 18.024s\n",
      "  training loss (in-iteration): \t0.343736\n",
      "  validation accuracy: \t\t\t87.41 %\n",
      "  learning rate: \t\t\t0.0100\n",
      "Epoch 31 of 100 took 17.914s\n",
      "  training loss (in-iteration): \t0.341271\n",
      "  validation accuracy: \t\t\t86.79 %\n",
      "  learning rate: \t\t\t0.0100\n",
      "Epoch 32 of 100 took 17.945s\n",
      "  training loss (in-iteration): \t0.322406\n",
      "  validation accuracy: \t\t\t85.55 %\n",
      "  learning rate: \t\t\t0.0100\n",
      "Epoch 33 of 100 took 18.007s\n",
      "  training loss (in-iteration): \t0.317600\n",
      "  validation accuracy: \t\t\t86.77 %\n",
      "  learning rate: \t\t\t0.0100\n",
      "Epoch 34 of 100 took 18.028s\n",
      "  training loss (in-iteration): \t0.306001\n",
      "  validation accuracy: \t\t\t87.39 %\n",
      "  learning rate: \t\t\t0.0099\n",
      "Epoch 35 of 100 took 17.927s\n",
      "  training loss (in-iteration): \t0.298550\n",
      "  validation accuracy: \t\t\t87.40 %\n",
      "  learning rate: \t\t\t0.0099\n",
      "Epoch 36 of 100 took 17.960s\n",
      "  training loss (in-iteration): \t0.296692\n",
      "  validation accuracy: \t\t\t86.72 %\n",
      "  learning rate: \t\t\t0.0098\n",
      "Epoch 37 of 100 took 18.044s\n",
      "  training loss (in-iteration): \t0.286067\n",
      "  validation accuracy: \t\t\t87.80 %\n",
      "  learning rate: \t\t\t0.0098\n",
      "Epoch 38 of 100 took 18.022s\n",
      "  training loss (in-iteration): \t0.281942\n",
      "  validation accuracy: \t\t\t87.85 %\n",
      "  learning rate: \t\t\t0.0097\n",
      "Epoch 39 of 100 took 17.914s\n",
      "  training loss (in-iteration): \t0.268970\n",
      "  validation accuracy: \t\t\t88.55 %\n",
      "  learning rate: \t\t\t0.0096\n",
      "Epoch 40 of 100 took 17.832s\n",
      "  training loss (in-iteration): \t0.270451\n",
      "  validation accuracy: \t\t\t87.88 %\n",
      "  learning rate: \t\t\t0.0095\n",
      "Epoch 41 of 100 took 17.885s\n",
      "  training loss (in-iteration): \t0.249569\n",
      "  validation accuracy: \t\t\t88.07 %\n",
      "  learning rate: \t\t\t0.0094\n",
      "Epoch 42 of 100 took 17.802s\n",
      "  training loss (in-iteration): \t0.250919\n",
      "  validation accuracy: \t\t\t87.84 %\n",
      "  learning rate: \t\t\t0.0093\n",
      "Epoch 43 of 100 took 17.973s\n",
      "  training loss (in-iteration): \t0.242458\n",
      "  validation accuracy: \t\t\t88.55 %\n",
      "  learning rate: \t\t\t0.0092\n",
      "Epoch 44 of 100 took 17.845s\n",
      "  training loss (in-iteration): \t0.235531\n",
      "  validation accuracy: \t\t\t87.99 %\n",
      "  learning rate: \t\t\t0.0090\n",
      "Epoch 45 of 100 took 18.123s\n",
      "  training loss (in-iteration): \t0.234761\n",
      "  validation accuracy: \t\t\t89.13 %\n",
      "  learning rate: \t\t\t0.0089\n",
      "Epoch 46 of 100 took 17.966s\n",
      "  training loss (in-iteration): \t0.227098\n",
      "  validation accuracy: \t\t\t87.38 %\n",
      "  learning rate: \t\t\t0.0088\n",
      "Epoch 47 of 100 took 17.935s\n",
      "  training loss (in-iteration): \t0.224860\n",
      "  validation accuracy: \t\t\t88.79 %\n",
      "  learning rate: \t\t\t0.0086\n",
      "Epoch 48 of 100 took 17.867s\n",
      "  training loss (in-iteration): \t0.207596\n",
      "  validation accuracy: \t\t\t88.77 %\n",
      "  learning rate: \t\t\t0.0085\n",
      "Epoch 49 of 100 took 17.746s\n",
      "  training loss (in-iteration): \t0.205492\n",
      "  validation accuracy: \t\t\t88.82 %\n",
      "  learning rate: \t\t\t0.0083\n",
      "Epoch 50 of 100 took 17.821s\n",
      "  training loss (in-iteration): \t0.198272\n",
      "  validation accuracy: \t\t\t88.33 %\n",
      "  learning rate: \t\t\t0.0081\n",
      "Epoch 51 of 100 took 17.832s\n",
      "  training loss (in-iteration): \t0.197435\n",
      "  validation accuracy: \t\t\t88.96 %\n",
      "  learning rate: \t\t\t0.0079\n",
      "Epoch 52 of 100 took 17.936s\n",
      "  training loss (in-iteration): \t0.191072\n",
      "  validation accuracy: \t\t\t89.42 %\n",
      "  learning rate: \t\t\t0.0078\n",
      "Epoch 53 of 100 took 18.012s\n",
      "  training loss (in-iteration): \t0.182230\n",
      "  validation accuracy: \t\t\t88.15 %\n",
      "  learning rate: \t\t\t0.0076\n",
      "Epoch 54 of 100 took 18.065s\n",
      "  training loss (in-iteration): \t0.182389\n",
      "  validation accuracy: \t\t\t88.63 %\n",
      "  learning rate: \t\t\t0.0074\n",
      "Epoch 55 of 100 took 17.961s\n",
      "  training loss (in-iteration): \t0.175655\n",
      "  validation accuracy: \t\t\t88.46 %\n",
      "  learning rate: \t\t\t0.0072\n",
      "Epoch 56 of 100 took 18.014s\n",
      "  training loss (in-iteration): \t0.166131\n",
      "  validation accuracy: \t\t\t89.36 %\n",
      "  learning rate: \t\t\t0.0070\n",
      "Epoch 57 of 100 took 17.933s\n",
      "  training loss (in-iteration): \t0.162298\n",
      "  validation accuracy: \t\t\t89.07 %\n",
      "  learning rate: \t\t\t0.0068\n",
      "Epoch 58 of 100 took 18.079s\n",
      "  training loss (in-iteration): \t0.154200\n",
      "  validation accuracy: \t\t\t89.47 %\n",
      "  learning rate: \t\t\t0.0065\n",
      "Epoch 59 of 100 took 17.953s\n",
      "  training loss (in-iteration): \t0.153158\n",
      "  validation accuracy: \t\t\t89.68 %\n",
      "  learning rate: \t\t\t0.0063\n",
      "Epoch 60 of 100 took 17.879s\n",
      "  training loss (in-iteration): \t0.139813\n",
      "  validation accuracy: \t\t\t89.60 %\n",
      "  learning rate: \t\t\t0.0061\n",
      "Epoch 61 of 100 took 17.982s\n",
      "  training loss (in-iteration): \t0.133963\n",
      "  validation accuracy: \t\t\t89.11 %\n",
      "  learning rate: \t\t\t0.0059\n",
      "Epoch 62 of 100 took 18.067s\n",
      "  training loss (in-iteration): \t0.135461\n",
      "  validation accuracy: \t\t\t89.54 %\n",
      "  learning rate: \t\t\t0.0057\n",
      "Epoch 63 of 100 took 17.939s\n",
      "  training loss (in-iteration): \t0.124064\n",
      "  validation accuracy: \t\t\t89.59 %\n",
      "  learning rate: \t\t\t0.0054\n",
      "Epoch 64 of 100 took 17.961s\n",
      "  training loss (in-iteration): \t0.118173\n",
      "  validation accuracy: \t\t\t89.14 %\n",
      "  learning rate: \t\t\t0.0052\n",
      "Epoch 65 of 100 took 18.000s\n",
      "  training loss (in-iteration): \t0.119192\n",
      "  validation accuracy: \t\t\t90.38 %\n",
      "  learning rate: \t\t\t0.0050\n",
      "Epoch 66 of 100 took 18.044s\n",
      "  training loss (in-iteration): \t0.110409\n",
      "  validation accuracy: \t\t\t89.82 %\n",
      "  learning rate: \t\t\t0.0048\n",
      "Epoch 67 of 100 took 17.850s\n",
      "  training loss (in-iteration): \t0.103491\n",
      "  validation accuracy: \t\t\t90.26 %\n",
      "  learning rate: \t\t\t0.0045\n",
      "Epoch 68 of 100 took 18.014s\n",
      "  training loss (in-iteration): \t0.099778\n",
      "  validation accuracy: \t\t\t89.89 %\n",
      "  learning rate: \t\t\t0.0043\n",
      "Epoch 69 of 100 took 17.903s\n",
      "  training loss (in-iteration): \t0.093966\n",
      "  validation accuracy: \t\t\t89.94 %\n",
      "  learning rate: \t\t\t0.0041\n",
      "Epoch 70 of 100 took 17.836s\n",
      "  training loss (in-iteration): \t0.090316\n",
      "  validation accuracy: \t\t\t90.24 %\n",
      "  learning rate: \t\t\t0.0039\n",
      "Epoch 71 of 100 took 17.969s\n",
      "  training loss (in-iteration): \t0.083369\n",
      "  validation accuracy: \t\t\t89.83 %\n",
      "  learning rate: \t\t\t0.0037\n",
      "Epoch 72 of 100 took 17.955s\n",
      "  training loss (in-iteration): \t0.080539\n",
      "  validation accuracy: \t\t\t91.08 %\n",
      "  learning rate: \t\t\t0.0035\n",
      "Epoch 73 of 100 took 17.875s\n",
      "  training loss (in-iteration): \t0.074426\n",
      "  validation accuracy: \t\t\t90.16 %\n",
      "  learning rate: \t\t\t0.0032\n",
      "Epoch 74 of 100 took 17.926s\n",
      "  training loss (in-iteration): \t0.070161\n",
      "  validation accuracy: \t\t\t90.73 %\n",
      "  learning rate: \t\t\t0.0030\n",
      "Epoch 75 of 100 took 17.908s\n",
      "  training loss (in-iteration): \t0.070783\n",
      "  validation accuracy: \t\t\t91.02 %\n",
      "  learning rate: \t\t\t0.0028\n",
      "Epoch 76 of 100 took 17.887s\n",
      "  training loss (in-iteration): \t0.061641\n",
      "  validation accuracy: \t\t\t90.53 %\n",
      "  learning rate: \t\t\t0.0026\n",
      "Epoch 77 of 100 took 18.088s\n",
      "  training loss (in-iteration): \t0.062760\n",
      "  validation accuracy: \t\t\t91.29 %\n",
      "  learning rate: \t\t\t0.0024\n",
      "Epoch 78 of 100 took 17.962s\n",
      "  training loss (in-iteration): \t0.052256\n",
      "  validation accuracy: \t\t\t91.07 %\n",
      "  learning rate: \t\t\t0.0022\n",
      "Epoch 79 of 100 took 18.003s\n",
      "  training loss (in-iteration): \t0.050607\n",
      "  validation accuracy: \t\t\t90.97 %\n",
      "  learning rate: \t\t\t0.0021\n",
      "Epoch 80 of 100 took 17.952s\n",
      "  training loss (in-iteration): \t0.049538\n",
      "  validation accuracy: \t\t\t91.40 %\n",
      "  learning rate: \t\t\t0.0019\n",
      "Epoch 81 of 100 took 17.957s\n",
      "  training loss (in-iteration): \t0.041620\n",
      "  validation accuracy: \t\t\t91.16 %\n",
      "  learning rate: \t\t\t0.0017\n",
      "Epoch 82 of 100 took 17.980s\n",
      "  training loss (in-iteration): \t0.041343\n",
      "  validation accuracy: \t\t\t91.58 %\n",
      "  learning rate: \t\t\t0.0015\n",
      "Epoch 83 of 100 took 17.882s\n",
      "  training loss (in-iteration): \t0.038187\n",
      "  validation accuracy: \t\t\t91.14 %\n",
      "  learning rate: \t\t\t0.0014\n",
      "Epoch 84 of 100 took 17.933s\n",
      "  training loss (in-iteration): \t0.036436\n",
      "  validation accuracy: \t\t\t91.52 %\n",
      "  learning rate: \t\t\t0.0012\n",
      "Epoch 85 of 100 took 18.000s\n",
      "  training loss (in-iteration): \t0.033564\n",
      "  validation accuracy: \t\t\t91.36 %\n",
      "  learning rate: \t\t\t0.0011\n",
      "Epoch 86 of 100 took 17.947s\n",
      "  training loss (in-iteration): \t0.031602\n",
      "  validation accuracy: \t\t\t91.52 %\n",
      "  learning rate: \t\t\t0.0010\n",
      "Epoch 87 of 100 took 18.069s\n",
      "  training loss (in-iteration): \t0.030459\n",
      "  validation accuracy: \t\t\t91.74 %\n",
      "  learning rate: \t\t\t0.0008\n",
      "Epoch 88 of 100 took 18.054s\n",
      "  training loss (in-iteration): \t0.029098\n",
      "  validation accuracy: \t\t\t91.86 %\n",
      "  learning rate: \t\t\t0.0007\n",
      "Epoch 89 of 100 took 18.079s\n",
      "  training loss (in-iteration): \t0.027266\n",
      "  validation accuracy: \t\t\t91.95 %\n",
      "  learning rate: \t\t\t0.0006\n",
      "Epoch 90 of 100 took 17.979s\n",
      "  training loss (in-iteration): \t0.026246\n",
      "  validation accuracy: \t\t\t91.64 %\n",
      "  learning rate: \t\t\t0.0005\n",
      "Epoch 91 of 100 took 17.959s\n",
      "  training loss (in-iteration): \t0.024027\n",
      "  validation accuracy: \t\t\t91.86 %\n",
      "  learning rate: \t\t\t0.0004\n",
      "Epoch 92 of 100 took 17.922s\n",
      "  training loss (in-iteration): \t0.023076\n",
      "  validation accuracy: \t\t\t91.84 %\n",
      "  learning rate: \t\t\t0.0003\n",
      "Epoch 93 of 100 took 17.896s\n",
      "  training loss (in-iteration): \t0.019996\n",
      "  validation accuracy: \t\t\t91.87 %\n",
      "  learning rate: \t\t\t0.0002\n",
      "Epoch 94 of 100 took 17.863s\n",
      "  training loss (in-iteration): \t0.022832\n",
      "  validation accuracy: \t\t\t91.87 %\n",
      "  learning rate: \t\t\t0.0002\n",
      "Epoch 95 of 100 took 17.823s\n",
      "  training loss (in-iteration): \t0.021696\n",
      "  validation accuracy: \t\t\t91.86 %\n",
      "  learning rate: \t\t\t0.0001\n",
      "Epoch 96 of 100 took 17.988s\n",
      "  training loss (in-iteration): \t0.022278\n",
      "  validation accuracy: \t\t\t92.05 %\n",
      "  learning rate: \t\t\t0.0001\n",
      "Epoch 97 of 100 took 17.932s\n",
      "  training loss (in-iteration): \t0.020189\n",
      "  validation accuracy: \t\t\t91.84 %\n",
      "  learning rate: \t\t\t0.0000\n",
      "Epoch 98 of 100 took 17.941s\n",
      "  training loss (in-iteration): \t0.020636\n",
      "  validation accuracy: \t\t\t91.77 %\n",
      "  learning rate: \t\t\t0.0000\n",
      "Epoch 99 of 100 took 17.939s\n",
      "  training loss (in-iteration): \t0.020020\n",
      "  validation accuracy: \t\t\t91.79 %\n",
      "  learning rate: \t\t\t0.0000\n",
      "Epoch 100 of 100 took 17.919s\n",
      "  training loss (in-iteration): \t0.021462\n",
      "  validation accuracy: \t\t\t91.90 %\n",
      "  learning rate: \t\t\t0.0000\n",
      "Final results:\n",
      "  test accuracy:\t\t90.82 %\n",
      "U'r freakin' amazin'!\n"
     ]
    }
   ],
   "source": [
    "# model_v1 = CNNv1().to(device)\n",
    "\n",
    "# n_epochs = 100\n",
    "# batch_size = 512\n",
    "# patience = 5\n",
    "# steps_per_epoch = len(train_loader)//batch_size + 1 if len(train_loader)%batch_size != 0 else len(train_loader)//batch_size\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model_v1.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=steps_per_epoch, epochs=n_epochs)\n",
    "\n",
    "# train(model_v1, optimizer, sch=scheduler, batch_size=batch_size, patience=patience, num_epochs=n_epochs, model_name='CNNv1', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNv1(\n",
       "  (activation): GELU()\n",
       "  (conv1): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv6): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv7): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv8): ConvBlock(\n",
       "    (batch_norm_2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): GELU()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v1_best = CNNv1().to(device)\n",
    "\n",
    "checkpoint = torch.load('models/CNNv1_95.pth')\n",
    "model_v1_best.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model_v1_best.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t90.87 %\n",
      "U'r freakin' amazin'!\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(model_v1_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Iteration path\n",
    "\n",
    "1. Выбрана модель, на которой была возможность быстро тестировать независимые от архитектуры фичи. С ее помощью были добавлены:\n",
    "    * Автоматическое сохранение параметров моделей с лучшим значением accuracy\n",
    "    * 1cycle policy\n",
    "    * Early stopping\n",
    "2. Собрана сеть из 8 сверточных слоев и одного полносвязного (CNNv1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}