## Materials
* Adaptive optimization methods (russian) - [video](https://yadi.sk/i/SAGl44PS3EHZeK)
* Deep learning frameworks (russian) - [video](https://www.youtube.com/watch?v=ghZyptkanB0) 

* Stochastic gradient descent modifications (english) - [video](https://www.youtube.com/watch?v=nhqo0u1a6fw)
* A blog post overview of gradient descent methods - [url](http://ruder.io/optimizing-gradient-descent/)
* Deep learning frameworks (english) - [video](https://www.youtube.com/watch?v=Vf_-OkqbwPo)


## More on adaptive optimization
* [Cool interactive demo of momentum](http://distill.pub/2017/momentum/)
* [wikipedia on SGD :)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), especially the "extensions and variants" section
* [RMSPROP video](https://www.youtube.com/watch?v=defQQqkXEfE)


## More on DL frameworks
  - A lecture on nonlinearities, initializations and other tricks in deep learning (karpathy) - [video](https://www.youtube.com/watch?v=GUtlrDbHhJM)
  - A lecture on activations, recap of adaptive SGD and dropout (karpathy) - [video](https://www.youtube.com/watch?v=KaR4lIdI1MQ)
  - [a deep learning neophite cheat sheet](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-1.html)
  - [bonus video] Deep learning philosophy: [our humble take](https://www.youtube.com/watch?v=9qyE1Ev1Xdw) (english)
  - [reading] on weight initialization: [blog post](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
  - [reading] pretty much all the module 1 of http://cs231n.github.io/
